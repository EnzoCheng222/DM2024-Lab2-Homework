# Student Information

- Name: é„­ä¸žæ© Enzo
- Student ID: 111701024 
- GitHub ID: EnzoCheng222
- Kaggle_name: EnzoCheng222
# 0.è³‡æ–™èˆ‡ç’°å¢ƒæº–å‚™
* èªªæ˜Žï¼šå› ç‚ºæ¯æ¬¡éƒ½è¦ä¸Šå‚³è³‡æ–™ï¼Œæ‰€ä»¥æˆ‘å¯«äº†ç°¡å–®çš„æ­¥é©Ÿï¼Œé€™æ¨£å¯ä»¥å¾ˆå¿«æŠŠç’°å¢ƒæº–å‚™å¥½

0. kaggle çš„é è¨­ç’°å¢ƒè¨­å®š 
1. å¾ž github ä¸‹è¼‰è³‡æ–™ https://github.com/cyiping/DM2024-Lab2-Master.git
2. åˆ‡æ›å·¥ä½œç›®éŒ„åˆ° /content/DM2024-Lab2-Master
3. é€£æŽ¥(mount) Google Drive
4. å¾ž google Drive æŠŠé å…ˆè¨“ç·´è³‡æ–™ GoogleNews-vectors-negative300.bin.gz æ‹·è²åˆ°å·¥ä½œç›®éŒ„





# é€™æ˜¯ kaggle çš„é è¨­ç’°å¢ƒè¨­å®š 

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
# å¾ž github ä¸‹è¼‰è³‡æ–™
!git clone https://github.com/cyiping/DM2024-Lab2-Master.git
# 2. åˆ‡æ›å·¥ä½œç›®éŒ„
import os
os.chdir('/kaggle/working/DM2024-Lab2-Master')

# ç¢ºèªä¸€ä¸‹
!pwd
!mkdir GoogleNews
!ls -al 
!pip install gdown
# å¾ž google Drive æŠŠé å…ˆè¨“ç·´è³‡æ–™ GoogleNews-vectors-negative300.bin.gz æ‹·è²åˆ°å·¥ä½œç›®éŒ„
import os

# 1. ç¢ºä¿ç›®æ¨™ç›®éŒ„å­˜åœ¨
os.makedirs('/kaggle/working/DM2024-Lab2-Master/GoogleNews', exist_ok=True)

# 2. ä½¿ç”¨ gdown ä¸‹è¼‰æ–‡ä»¶åˆ°ç›®æ¨™ç›®éŒ„
!gdown 'https://drive.google.com/uc?id=1qqLXNv3iv-27o6snpVO99F2mbuMlvyrL' -O /kaggle/working/DM2024-Lab2-Master/GoogleNews/GoogleNews-vectors-negative300.bin.gz

# 1. Lab 2 Exercise
- home exercises in the DM2024-Lab2-master Repo
---
### **>>> Exercise 1 (Take home): **  
Plot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)

!pip install pandas
import pandas as pd

### training data
anger_train = pd.read_csv("data/semeval/train/anger-ratings-0to1.train.txt",
                         sep="\t", header=None,names=["id", "text", "emotion", "intensity"])
sadness_train = pd.read_csv("data/semeval/train/sadness-ratings-0to1.train.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
fear_train = pd.read_csv("data/semeval/train/fear-ratings-0to1.train.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
joy_train = pd.read_csv("data/semeval/train/joy-ratings-0to1.train.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
anger_train
# combine 4 sub-dataset
train_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)
train_df
### testing data
anger_test = pd.read_csv("data/semeval/dev/anger-ratings-0to1.dev.gold.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
sadness_test = pd.read_csv("data/semeval/dev/sadness-ratings-0to1.dev.gold.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
fear_test = pd.read_csv("data/semeval/dev/fear-ratings-0to1.dev.gold.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
joy_test = pd.read_csv("data/semeval/dev/joy-ratings-0to1.dev.gold.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])

# combine 4 sub-dataset
test_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)
train_df.head()
# shuffle dataset éš¨æ©Ÿæ‰“äº‚ï¼ˆshuffleï¼‰è³‡æ–™é›†çš„é †åº
train_df = train_df.sample(frac=1)
test_df = test_df.sample(frac=1)
print('ok')
print("Shape of Training df: ", train_df.shape)
print("Shape of Testing df: ", test_df.shape)
# Answer :
# å‡ºç¾é »åº¦ä¸Šä½ 30 å€‹ã®å˜èªžã‚’è¦‹ã¤ã‘ã¦ã€çµµã‚’æã„ã¦æç¤ºã—ã¾ã™ã€‚

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer

# train = pd.DataFrame({'text': [...]})  # Load your train dataset here
# test = pd.DataFrame({'text': [...]})   # Load your test dataset here

train = train_df
test = test_df

# Function to calculate and plot top 30 word frequencies

def plot_top_words(data, dataset_name, top_n=30):
    # Initialize the CountVectorizer
    vectorizer = CountVectorizer()
    # Fit and transform the text data
    word_counts = vectorizer.fit_transform(data['text'])
    # Sum up the occurrences of each word
    word_sums = word_counts.sum(axis=0)
    # Get the words and their corresponding counts
    words_freq = [(word, word_sums[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
    # Sort words by frequency
    sorted_words = sorted(words_freq, key=lambda x: x[1], reverse=True)[:top_n]
    # Split words and counts for plotting
    words, counts = zip(*sorted_words)

# Plotting
    plt.figure(figsize=(12, 6))
    plt.bar(words, counts)
    plt.xticks(rotation=45, ha='right')
    plt.title(f'Top {top_n} Word Frequencies in {dataset_name} Dataset')
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.show()

# Usage
plot_top_words(train, 'Train')
plot_top_words(test, 'Test')



---
###  1.2 Save data
We will save our data in Pickle format. The pickle module implements binary protocols for serializing and de-serializing a Python object structure.   
  
Some advantages for using pickle structure:  
* Because it stores the attribute type, it's more convenient for cross-platform use.  
* When your data is huge, it could use less space to store also consume less loading time.
## save to pickle file
train_df.to_pickle("train_df.pkl")
test_df.to_pickle("test_df.pkl")
import pandas as pd

## load a pickle file
train_df = pd.read_pickle("train_df.pkl")
test_df = pd.read_pickle("test_df.pkl")
---
### 1.3 Exploratory data analysis (EDA)

Again, before getting our hands dirty, we need to explore a little bit and understand the data we're dealing with.
# group to find distribution
train_df.groupby(['emotion']).count()['text']
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

# the histogram of the data
labels = train_df['emotion'].unique()
post_total = len(train_df)
df1 = train_df.groupby(['emotion']).count()['text']
df1 = df1.apply(lambda x: round(x*100/post_total,3))

#plot
fig, ax = plt.subplots(figsize=(5,3))
plt.bar(df1.index,df1.values)

#arrange
plt.ylabel('% of instances')
plt.xlabel('Emotion')
plt.title('Emotion distribution')
plt.grid(True)
plt.show()
## 2. Feature engineering
### Using Bag of Words
Using scikit-learn ```CountVectorizer``` perform word frequency and use these as features to train a model.  
http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
from sklearn.feature_extraction.text import CountVectorizer
# build analyzers (bag-of-words)
BOW_vectorizer = CountVectorizer()
# 1. Learn a vocabulary dictionary of all tokens in the raw documents.
BOW_vectorizer.fit(train_df['text'])

# 2. Transform documents to document-term matrix.
train_data_BOW_features = BOW_vectorizer.transform(train_df['text'])
test_data_BOW_features = BOW_vectorizer.transform(test_df['text'])
# check the result
train_data_BOW_features
type(train_data_BOW_features)
# add .toarray() to show
train_data_BOW_features.toarray()
# check the dimension
train_data_BOW_features.shape
# observe some feature names
feature_names = BOW_vectorizer.get_feature_names_out()
feature_names[100:110]
The embedding is done. We can technically feed this into our model. However, depending on the embedding technique you use and your model, your accuracy might not be as high, because:

* curse of dimensionality  (we have 10,115 dimension now)
* some important features are ignored (for example, some models using emoticons yeld better performance than counterparts)
"ðŸ˜‚" in feature_names
## Let's try using another tokenizer below.
import nltk
print(nltk.data.path)
!pip install --upgrade nltk
# æ•´åˆæˆåŠŸç‰ˆæœ¬
# è™•ç† nltk è³‡æ–™è·¯å¾‘å•é¡Œ
import nltk

# å¦‚æžœæœ‰å•é¡Œï¼Œå¯ä»¥æŒ‡å®š nltk è³‡æ–™è·¯å¾‘
nltk.data.path.append('/path/to/nltk_data')  # æ›¿æ›ç‚ºé©åˆçš„è·¯å¾‘
nltk.download('punkt')  # ç¢ºä¿ punkt è³‡æ–™ä¸‹è¼‰å®Œæˆ

# å¦‚æžœ nltk çš„å…§å»ºåˆ†è©žå™¨å•é¡ŒæŒçºŒï¼Œå‰‡ä½¿ç”¨è‡ªè¨‚åˆ†è©žå™¨
import re
from sklearn.feature_extraction.text import CountVectorizer

# å®šç¾©è‡ªè¨‚åˆ†è©žå™¨
def simple_tokenizer(text):
    """
    è‡ªè¨‚ç°¡å–®çš„åˆ†è©žå™¨ï¼šå°‡æ–‡å­—è½‰ç‚ºå°å¯«ï¼Œä¸¦ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–å–®å­—
    """
    return re.findall(r'\b\w+\b', text.lower())

# åˆå§‹åŒ– CountVectorizerï¼Œä½¿ç”¨è‡ªè¨‚åˆ†è©žå™¨
BOW_500 = CountVectorizer(max_features=500, tokenizer=simple_tokenizer)

# å‡è¨­ train_df æ˜¯ä½ çš„è¨“ç·´è³‡æ–™ï¼ŒåŒ…å« 'text' æ¬„ä½
BOW_500.fit(train_df['text'])  # æ“¬åˆè¨“ç·´è³‡æ–™
train_data_BOW_features_500 = BOW_500.transform(train_df['text'])  # è½‰æ›ç‚ºç‰¹å¾µçŸ©é™£

# ç¢ºèªç‰¹å¾µçŸ©é™£çš„ç¶­åº¦
print(train_data_BOW_features_500.shape)  # é¡¯ç¤º (æ¨£æœ¬æ•¸, ç‰¹å¾µæ•¸)
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
import nltk
import re

# è¼‰å…¥æ•¸æ“šï¼ˆè«‹æ ¹æ“šä½ çš„å¯¦éš›æ•¸æ“šè·¯å¾‘ä¿®æ”¹ï¼‰
train_df = pd.read_pickle("train_df.pkl")

# å®šç¾©è‡ªå®šç¾©åˆ†è©žå™¨
def custom_tokenizer(text):
    """
    è‡ªè¨‚åˆ†è©žå™¨ï¼šå°‡æ–‡å­—è½‰ç‚ºå°å¯«ï¼Œä¸¦ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–å–®å­—å’Œæ¨™é»žç¬¦è™Ÿ
    """
    tokens = re.findall(r'\b\w+\b|[^\w\s]', text.lower())
    return tokens

# åˆå§‹åŒ– CountVectorizerï¼Œä½¿ç”¨è‡ªå®šç¾©åˆ†è©žå™¨å’Œè¨­å®šç‰¹å¾µæ•¸é‡
BOW_500 = CountVectorizer(max_features=500, tokenizer=custom_tokenizer)

# æ“¬åˆè¨“ç·´è³‡æ–™
BOW_500.fit(train_df['text'])

# è½‰æ›ç‚ºç‰¹å¾µçŸ©é™£
train_data_BOW_features_500 = BOW_500.transform(train_df['text'])

# é¡¯ç¤ºç‰¹å¾µçŸ©é™£çš„ç¶­åº¦å’Œéƒ¨åˆ†å…§å®¹ï¼ˆç”¨æ–¼ç¢ºèªçµæžœï¼‰
print("ç‰¹å¾µçŸ©é™£çš„ç¶­åº¦ï¼š", train_data_BOW_features_500.shape)
print("ç‰¹å¾µçŸ©é™£çš„éƒ¨åˆ†å…§å®¹ï¼š")
# print(train_data_BOW_features_500.toarray()[:5]) # é¡¯ç¤ºå‰ 5 ç­†æ•¸æ“šçš„ç‰¹å¾µ

train_data_BOW_features_500.toarray()
train_data_BOW_features_500.toarray()
# observe some feature names
feature_names_500 = BOW_500.get_feature_names_out()
feature_names_500[100:110]
"ðŸ˜‚" in feature_names_500
---
### ** >>> Exercise 2 (Take home): **  
Generate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110].
train_df['text']
# Answer here
from sklearn.feature_extraction.text import TfidfVectorizer

# å‡è¨­ train_df å·²ç¶“è¼‰å…¥ä¸¦ä¸”åŒ…å«ä¸€å€‹åç‚º 'text' çš„æ¬„ä½ï¼Œè©²æ¬„ä½å­˜æ”¾æ–‡æœ¬æ•¸æ“š
# æ­¤è™•çš„ train_df æ‡‰ç‚ºå¯¦éš›è³‡æ–™é›†çš„ DataFrame

# å®šç¾© TF-IDF å‘é‡ç”Ÿæˆå™¨ä¸¦è¨­å®š 1000 å€‹ç‰¹å¾µ
tfidf_vectorizer = TfidfVectorizer(max_features=1000)

# ä½¿ç”¨ TF-IDF å‘é‡ç”Ÿæˆå™¨æ“¬åˆè¨“ç·´è³‡æ–™çš„ 'text' æ¬„ä½ï¼Œç”Ÿæˆç‰¹å¾µå‘é‡
tfidf_vectorizer.fit(train_df['text'])

# å–å¾—ç‰¹å¾µåç¨±ï¼Œä¸¦é¡¯ç¤ºç´¢å¼• 100 åˆ° 110 çš„ç‰¹å¾µåç¨±
feature_names = tfidf_vectorizer.get_feature_names_out()[100:110]

# é¡¯ç¤ºç‰¹å¾µåç¨±çš„è¼¸å‡º
print(feature_names)

---
## 3. Model
### 3.1 Decision Trees
Using scikit-learn ```DecisionTreeClassifier``` performs word frequency and uses these as features to train a model.  
http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier
from sklearn.tree import DecisionTreeClassifier

# for a classificaiton problem, you need to provide both training & testing data
X_train = BOW_500.transform(train_df['text'])
y_train = train_df['emotion']

X_test = BOW_500.transform(test_df['text'])
y_test = test_df['emotion']

## take a look at data dimension is a good habit  :)
print('X_train.shape: ', X_train.shape)
print('y_train.shape: ', y_train.shape)
print('X_test.shape: ', X_test.shape)
print('y_test.shape: ', y_test.shape)
## build DecisionTree model
DT_model = DecisionTreeClassifier(random_state=1)

## training!
DT_model = DT_model.fit(X_train, y_train)

## predict!
y_train_pred = DT_model.predict(X_train)
y_test_pred = DT_model.predict(X_test)

## so we get the pred result
y_test_pred[:10]
---
## 4. Results Evaluation
Now we will check the results of our model's performance
## accuracy
from sklearn.metrics import accuracy_score

acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)
acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)

print('training accuracy: {}'.format(round(acc_train, 2)))
print('testing accuracy: {}'.format(round(acc_test, 2)))
## precision, recall, f1-score,
from sklearn.metrics import classification_report

print(classification_report(y_true=y_test, y_pred=y_test_pred))
## check by confusion matrix
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred)
print(cm)
# Funciton for visualizing confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import itertools

def plot_confusion_matrix(cm, classes, title='Confusion matrix',
                          cmap=sns.cubehelix_palette(as_cmap=True)):
    """
    This function is modified from:
    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html
    """
    classes.sort()
    tick_marks = np.arange(len(classes))

    fig, ax = plt.subplots(figsize=(5,5))
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels = classes,
           yticklabels = classes,
           title = title,
           xlabel = 'Predicted label',
           ylabel = 'True label')

    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment="center", color="white" if cm[i, j] > thresh else "black")
    ylim_top = len(classes) - 0.5
    plt.ylim([ylim_top, -.5])
    plt.tight_layout()
    plt.show()

# plot your confusion matrix
my_tags = ['anger', 'fear', 'joy', 'sadness']
plot_confusion_matrix(cm, classes=my_tags, title='Confusion matrix')
---
### ** >>> Exercise 3 (Take home): **  
Can you interpret the results above? What do they mean?
# Answer here

### Analysis of Results

#### 1. **Confusion Matrix**
The confusion matrix displays the model's performance in classifying emotions:

| **True Label / Predicted Label** | Anger | Fear | Joy | Sadness |
|----------------------------------|-------|------|-----|---------|
| **Anger**                        | 51    | 14   | 10  | 9       |
| **Fear**                         | 9     | 82   | 10  | 9       |
| **Joy**                          | 9     | 7    | 55  | 8       |
| **Sadness**                      | 7     | 15   | 10  | 42      |

Key Observations:
- Most "Fear" samples are correctly predicted (82 correct out of 110).
- "Sadness" is often misclassified as "Fear" (15 cases).

#### 2. **Accuracy**
- **Training Accuracy:** `0.99`
  - Extremely high, indicating overfitting.
- **Testing Accuracy:** `0.66`
  - Significant drop, showing poor generalization on unseen data.

#### 3. **Precision, Recall, and F1-Score**

| **Class**   | **Precision** | **Recall** | **F1-Score** | **Support** |
|-------------|---------------|------------|--------------|-------------|
| Anger       | 0.67          | 0.61       | 0.64         | 84          |
| Fear        | 0.69          | 0.75       | 0.72         | 110         |
| Joy         | 0.65          | 0.70       | 0.67         | 79          |
| Sadness     | 0.62          | 0.57       | 0.59         | 74          |
| **Macro Avg** | 0.66          | 0.65       | 0.65         | 347         |
| **Weighted Avg** | 0.66      | 0.66       | 0.66         | 347         |

Key Points:
- "Fear" has the best performance (F1-score: 0.72).
- "Sadness" performs the worst (F1-score: 0.59).

#### 4. **Overfitting Indicator**
- Training accuracy (0.99) far exceeds testing accuracy (0.66).
- The model memorizes training data but struggles to generalize.

---

### Recommendations

1. **Overfitting Mitigation**
   - Add regularization (e.g., dropout, L2 regularization).
   - Use early stopping during training.
   - Reduce model complexity if applicable.

2. **Class Imbalance Handling**
   - Augment data for minority classes like "Sadness."
   - Use class weights in the loss function to emphasize underperforming classes.

3. **Evaluation and Validation**
   - Implement k-fold cross-validation to improve model evaluation.
   - Use stratified sampling to ensure balanced splits during training.

4. **Hyperparameter Tuning**
   - Experiment with learning rates, batch sizes, and optimizers.
   - Consider alternative model architectures (e.g., transformer-based models).

5. **Performance Metrics**
   - Monitor precision and recall trends for minority classes to ensure improvements are balanced across all labels.

### Explanation of the Confusion Matrix

This diagram is a confusion matrix, commonly used to evaluate the performance of classification models. Below is an explanation of the chart:

#### Structure of the Confusion Matrix:
- **Vertical Axis (True Label):**
  - Represents the actual labels, indicating the true emotional categories of the data (`anger`, `fear`, `joy`, `sadness`).
- **Horizontal Axis (Predicted Label):**
  - Represents the predicted labels, indicating the emotion categories predicted by the model.
- **Numeric Values:**
  - Each cell in the matrix represents the number of samples for a particular classification. For example:
    - The top-left number "61" indicates 61 samples with the true label `anger` were correctly predicted as `anger`.

#### Key Observations:
1. **Correct Classifications (Diagonal Numbers):**
   - Numbers on the diagonal represent the correctly classified samples. For example:
     - `anger`: 61 samples were correctly classified as `anger`.
     - `fear`: 78 samples were correctly classified as `fear`.
     - Similarly for other categories.

2. **Misclassifications (Off-Diagonal Numbers):**
   - Numbers outside the diagonal indicate incorrect classifications. For example:
     - 9 samples with the true label `anger` were misclassified as `joy`.
     - 14 samples with the true label `fear` were misclassified as `anger`.
   - These values indicate where the model tends to confuse one class with another.

#### Performance Summary:
- **Best Performance:**
  - The model performs best on `fear`, with 78 samples correctly classified.
- **High Error Rate:**
  - The classification for `sadness` shows the highest confusion, with 15 samples misclassified as `anger`.

#### Conclusion:
The confusion matrix reveals the overall performance of the model. Most samples are correctly classified, but there is noticeable confusion between certain emotion categories. For example:
- The model struggles to differentiate between `anger` and `fear`.
- Similarly, there is confusion between `sadness` and `anger`.

#### Optimization Directions:
- Improve feature selection to better distinguish between similar classes.
- Consider adopting a more complex model or fine-tuning the existing one to reduce misclassification and enhance overall classification accuracy.

---
### ** >>> Exercise 4 (Take home): **  
Build a model using a ```Naive Bayes``` model and train it. What are the testing results?

*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html
#### [Answer here]
### å–®ç´”è²æ°(Naive Bayes) æ¨¡åž‹é€šå¸¸é©åˆç”¨æ–¼æ–‡æœ¬åˆ†é¡žï¼Œä¸”åœ¨é«˜ç¶­æ•¸æ“šä¸Šæ•ˆæžœè‰¯å¥½ã€‚
* æº–ç¢ºçŽ‡ (Accuracy)ï¼šé¡¯ç¤ºæ¨¡åž‹åœ¨æ¸¬è©¦é›†ä¸Šæ•´é«”çš„æº–ç¢ºæ€§ã€‚
* åˆ†é¡žå ±å‘Š (Classification Report)ï¼šæä¾›æ¯å€‹æƒ…ç·’é¡žåˆ¥çš„ç²¾ç¢ºçŽ‡ (precision)ã€å¬å›žçŽ‡ (recall) å’Œ F1 åˆ†æ•¸ï¼Œæ–¹ä¾¿æª¢æŸ¥æ¨¡åž‹åœ¨æ¯å€‹é¡žåˆ¥çš„è¡¨ç¾ã€‚
* æ··æ·†çŸ©é™£ (Confusion Matrix)ï¼šé¡¯ç¤ºæ¨¡åž‹åœ¨å„æƒ…ç·’é¡žåˆ¥ä¸Šçš„æ­£ç¢ºåˆ†é¡žèˆ‡éŒ¯èª¤åˆ†é¡žæƒ…æ³ï¼Œå¹«åŠ©è§€å¯Ÿæ¨¡åž‹åœ¨å“ªäº›æƒ…ç·’é¡žåˆ¥ä¹‹é–“æ˜“å‡ºç¾æ··æ·†ã€‚
print(train_df.columns)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

# å‡è¨­ train_df å·²ç¶“è¼‰å…¥ä¸¦ä¸”åŒ…å« 'text' å’Œ 'emotion' æ¬„ä½ï¼Œåˆ†åˆ¥è¡¨ç¤ºæ–‡æœ¬æ•¸æ“šå’Œå…¶å°æ‡‰çš„æƒ…ç·’æ¨™ç±¤

# ç¬¬ä¸€æ­¥ï¼šæº–å‚™æ•¸æ“š
# å°‡æ•¸æ“šåˆ†ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†ï¼ˆä¾‹å¦‚ï¼Œ80% ç”¨æ–¼è¨“ç·´ï¼Œ20% ç”¨æ–¼æ¸¬è©¦ï¼‰
X_train, X_test, y_train, y_test = train_test_split(train_df['text'], train_df['emotion'], test_size=0.2, random_state=42)

# ç¬¬äºŒæ­¥ï¼šå°‡æ–‡æœ¬æ•¸æ“šè½‰æ›ç‚º TF-IDF ç‰¹å¾µ
# ä½¿ç”¨ 1000 å€‹ç‰¹å¾µï¼Œèˆ‡ä¹‹å‰çš„ç·´ç¿’ä¿æŒä¸€è‡´
tfidf_vectorizer = TfidfVectorizer(max_features=1000)

# å°‡è¨“ç·´æ•¸æ“šé€²è¡Œæ“¬åˆä¸¦è½‰æ›ç‚º TF-IDF ç‰¹å¾µçŸ©é™£ï¼Œæ¸¬è©¦æ•¸æ“šé€²è¡Œè½‰æ›
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# ç¬¬ä¸‰æ­¥ï¼šè¨“ç·´ Naive Bayes æ¨¡åž‹
# ä½¿ç”¨ MultinomialNB æ¨¡åž‹ï¼Œé€™ç¨®æ¨¡åž‹é©åˆæ–‡æœ¬åˆ†é¡ž
nb_model = MultinomialNB()

# åœ¨è¨“ç·´æ•¸æ“šä¸Šæ“¬åˆæ¨¡åž‹
nb_model.fit(X_train_tfidf, y_train)

# ç¬¬å››æ­¥ï¼šåœ¨æ¸¬è©¦é›†ä¸Šé€²è¡Œé æ¸¬
y_pred = nb_model.predict(X_test_tfidf)

# ç¬¬äº”æ­¥ï¼šè©•ä¼°æ¨¡åž‹
# è¨ˆç®—æº–ç¢ºçŽ‡ã€åˆ†é¡žå ±å‘Šå’Œæ··æ·†çŸ©é™£
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)

# é¡¯ç¤ºçµæžœ
print("æº–ç¢ºçŽ‡:", accuracy)
print("\nåˆ†é¡žå ±å‘Š:\n", classification_rep)
print("\næ··æ·†çŸ©é™£:\n", confusion_mat)

# ç¬¬å…­æ­¥ï¼šç¹ªè£½æ··æ·†çŸ©é™£
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='YlOrRd', xticklabels=nb_model.classes_, yticklabels=nb_model.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

### Analysis of Results
### Analysis of Results

#### 1. **Overall Accuracy**
- **Accuracy:** `0.7773`
  - The model achieves an accuracy of approximately 77.7% on the dataset, which indicates reasonably good performance for a multi-class classification task.

---

#### 2. **Classification Report**

| **Class**   | **Precision** | **Recall** | **F1-Score** | **Support** |
|-------------|---------------|------------|--------------|-------------|
| Anger       | 0.81          | 0.76       | 0.78         | 173         |
| Fear        | 0.71          | 0.93       | 0.80         | 245         |
| Joy         | 0.89          | 0.69       | 0.77         | 150         |
| Sadness     | 0.80          | 0.65       | 0.72         | 155         |

- **Precision:** Measures how many predictions for each class were correct.
  - `Joy` has the highest precision (0.89), meaning it rarely predicts `Joy` incorrectly.
  - `Fear` has the lowest precision (0.71), indicating it predicts `Fear` less precisely compared to other classes.
  
- **Recall:** Measures how well the model identifies all true samples for each class.
  - `Fear` has the highest recall (0.93), meaning most true `Fear` samples are identified correctly.
  - `Sadness` has the lowest recall (0.65), suggesting many true `Sadness` samples are misclassified.

- **F1-Score:** A harmonic mean of precision and recall.
  - The highest F1-Score is for `Fear` (0.80), indicating it is the most balanced in terms of precision and recall.
  - The lowest F1-Score is for `Sadness` (0.72), suggesting room for improvement in both precision and recall.

- **Macro Average:**
  - `Precision`: 0.80
  - `Recall`: 0.76
  - `F1-Score`: 0.77
  - These scores treat all classes equally, providing an overall measure of the model's balance across classes.

- **Weighted Average:**
  - `Precision`: 0.79
  - `Recall`: 0.78
  - `F1-Score`: 0.77
  - These scores take into account the number of samples in each class, reflecting the overall performance weighted by class size.

---

#### 3. **Confusion Matrix**

| **True Label / Predicted Label** | Anger | Fear | Joy | Sadness |
|----------------------------------|-------|------|-----|---------|
| **Anger**                        | 131   | 27   | 5   | 10      |
| **Fear**                         | 9     | 227  | 3   | 6       |
| **Joy**                          | 8     | 29   | 103 | 10      |
| **Sadness**                      | 13    | 36   | 5   | 101     |

**Key Observations:**
1. **Correct Classifications (Diagonal):**
   - `Anger`: Correctly classified 131 out of 173 samples (76%).
   - `Fear`: Correctly classified 227 out of 245 samples (93%).
   - `Joy`: Correctly classified 103 out of 150 samples (69%).
   - `Sadness`: Correctly classified 101 out of 155 samples (65%).

2. **Misclassifications (Off-Diagonal):**
   - `Anger` is often misclassified as `Fear` (27 samples).
   - `Sadness` is frequently misclassified as `Fear` (36 samples), indicating confusion between these classes.
   - `Joy` is occasionally misclassified as `Fear` (29 samples).

---

### Key Insights and Recommendations

#### 1. **Strengths:**
- The model performs well on the `Fear` class, with high recall (0.93) and F1-score (0.80).
- Overall accuracy (77.7%) indicates reasonable performance on this multi-class classification task.

#### 2. **Weaknesses:**
- Significant confusion exists between `Fear` and other classes, especially `Sadness`.
- The `Joy` and `Sadness` classes show lower recall, indicating many true samples are misclassified.

#### 3. **Recommendations for Improvement:**
1. **Handle Class Confusion:**
   - Investigate feature overlap between `Fear` and `Sadness` to reduce misclassifications.
   - Consider more granular emotion features or embeddings to better distinguish between these emotions.

2. **Improve Recall for Weak Classes:**
   - Augment data for `Sadness` and `Joy` to improve model recognition of these classes.
   - Experiment with loss function weighting to prioritize underperforming classes.

3. **Regularization and Hyperparameter Tuning:**
   - Introduce regularization techniques like dropout or L2 regularization to reduce noise in predictions.
   - Tune hyperparameters such as learning rate, batch size, and optimizer.

4. **Advanced Models:**
   - Explore transformer-based models (e.g., BERT, RoBERTa) for better contextual understanding.
   - Incorporate domain-specific embeddings if available.

5. **Cross-Validation:**
   - Use k-fold cross-validation to validate performance consistency across different splits of the dataset.
 cross-validation to validate performance consistency across different splits of the dataset.

---
### ** >>> Exercise 5 (Take home): **  

How do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences.
### Comparison of Naive Bayes and Decision Tree Models

The comparison between the Naive Bayes model and the Decision Tree model highlights differences in accuracy, stability, and interpretability. Below is a detailed analysis of these differences:

---

### Results Comparison

1. **Accuracy and Performance**:
   - **Naive Bayes**: 
     - Performs well on text classification tasks, especially with high-dimensional and sparse data.
     - Relies on the conditional independence assumption, which simplifies computation and makes it efficient at handling large numbers of features.
   - **Decision Tree**:
     - May perform worse on such datasets, particularly when there are complex relationships between features that it struggles to capture.

2. **Error Rate and Class Differentiation**:
   - **Naive Bayes**:
     - Offers more stable performance in multi-class problems, with fewer misclassifications due to its probabilistic approach.
   - **Decision Tree**:
     - Tends to have higher misclassification rates, particularly for borderline classes or imbalanced datasets.
     - Can be sensitive to data distribution and prone to overfitting.

3. **Interpretability**:
   - **Decision Tree**:
     - Highly interpretable, as it provides a clear decision path, making it easier to understand how the model classifies each sample.
   - **Naive Bayes**:
     - Less interpretable since it relies purely on probability calculations without a visual decision process.

---

### Theoretical Background

1. **Naive Bayes Model**:
   - Based on the assumption of conditional independence, where each feature is assumed to be independent given the class.
   - While this assumption might not always hold true, it works well for high-dimensional text data by simplifying computations.
   - **Advantages**:
     - Suitable for high-dimensional and sparse data.
     - Fast training and prediction.
     - Less prone to overfitting.
   - **Disadvantages**:
     - Performance decreases when strong dependencies exist between features.

2. **Decision Tree Model**:
   - Operates by recursively splitting the data based on feature importance, creating a tree-like structure that defines the classification rules.
   - Handles complex relationships between features effectively.
   - **Advantages**:
     - Highly interpretable.
     - Can capture feature dependencies.
     - Suitable for low- or medium-dimensional data.
   - **Disadvantages**:
     - Prone to overfitting.
     - Unstable on high-dimensional or sparse data.
     - Performs poorly on imbalanced datasets.

---

### Conclusion

In summary:
- **Naive Bayes** excels in high-dimensional, sparse text classification tasks due to its ability to handle large numbers of features and avoid overfitting.
- **Decision Tree** is better suited for datasets with fewer features and strong dependencies among them.

The choice of the model depends on the dataset's characteristics. Selecting the right model ensures optimal classification performance.

---
## 6. Deep Learning

We use [Keras](https://keras.io/) to be our deep learning framework, and follow the [Model (functional API)](https://keras.io/models/model/) to build a Deep Neural Network (DNN) model. Keras runs with Tensorflow in the backend. It's a nice abstraction to start working with NN models.

Because Deep Learning is a 1-semester course, we can't talk about each detail about it in the lab session. Here, we only provide a simple template about how to build & run a DL model successfully. You can follow this template to design your model.

We will begin by building a fully connected network, which looks like this:
### 6.1 Prepare data (X, y)
### tensorflow å®‰è£å®Œç•¢ä¹‹å¾Œéœ€è¦é‡å•Ÿ kernel
é‡å•Ÿä¹‹å¾Œé€™ä¸€æ®µä¸ç”¨åŸ·è¡Œ
!pip install keras
!pip install tensorflow
import keras

# standardize name (X, y)
X_train = BOW_500.transform(train_df['text'])
y_train = train_df['emotion']

X_test = BOW_500.transform(test_df['text'])
y_test = test_df['emotion']

## check dimension is a good habbit
print('X_train.shape: ', X_train.shape)
print('y_train.shape: ', y_train.shape)
print('X_test.shape: ', X_test.shape)
print('y_test.shape: ', y_test.shape)
BOW_500.transform(test_df['text'])
### 6.2 Deal with categorical label (y)

Rather than put your label `train_df['emotion']` directly into a model, we have to process these categorical (or say nominal) label by ourselves.

Here, we use the basic method [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) to transform our categorical  labels to numerical ones.
## deal with label (string -> one-hot)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
label_encoder.fit(y_train)
print('check label: ', label_encoder.classes_)
print('\n## Before convert')
print('y_train[0:4]:\n', y_train[0:4])
print('\ny_train.shape: ', y_train.shape)
print('y_test.shape: ', y_test.shape)

def label_encode(le, labels):
    enc = le.transform(labels)
    return keras.utils.to_categorical(enc)

def label_decode(le, one_hot_label):
    dec = np.argmax(one_hot_label, axis=1)
    return le.inverse_transform(dec)

y_train = label_encode(label_encoder, y_train)
y_test = label_encode(label_encoder, y_test)

print('\n\n## After convert')
print('y_train[0:4]:\n', y_train[0:4])
print('\ny_train.shape: ', y_train.shape)
print('y_test.shape: ', y_test.shape)
### 6.3 Build model
# I/O check
input_shape = X_train.shape[1]
print('input_shape: ', input_shape)

output_shape = len(label_encoder.classes_)
print('output_shape: ', output_shape)
from keras.models import Model
from keras.layers import Input, Dense
from keras.layers import ReLU, Softmax

# input layer
model_input = Input(shape=(input_shape, ))  # 500
X = model_input

# 1st hidden layer
X_W1 = Dense(units=64)(X)  # 64
H1 = ReLU()(X_W1)

# 2nd hidden layer
H1_W2 = Dense(units=64)(H1)  # 64
H2 = ReLU()(H1_W2)

# output layer
H2_W3 = Dense(units=output_shape)(H2)  # 4
H3 = Softmax()(H2_W3)

model_output = H3

# create model
model = Model(inputs=[model_input], outputs=[model_output])

# loss function & optimizer
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# show model construction
model.summary()
### 6.4 Train
from keras.callbacks import CSVLogger

csv_logger = CSVLogger('logs/training_log.csv')

# training setting
epochs = 25
batch_size = 32

# training!
history = model.fit(X_train, y_train,
                    epochs=epochs,
                    batch_size=batch_size,
                    callbacks=[csv_logger],
                    validation_data = (X_test, y_test))
print('training finish')
### 6.5 Predict on testing data
## predict
pred_result = model.predict(X_test, batch_size=128)
pred_result[:5]
pred_result = label_decode(label_encoder, pred_result)
pred_result[:5]
from sklearn.metrics import accuracy_score

print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))
# Let's take a look at the training log
training_log = pd.DataFrame()
training_log = pd.read_csv("logs/training_log.csv")
training_log
---
### ** >>> Exercise 6 (Take home): **  

Plot the Training and Validation Accuracy and Loss (different plots), just like the images below.(Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?
from IPython.display import Image
Image('/kaggle/working/DM2024-Lab2-Master/pics/pic3.png')
from IPython.display import Image
Image('/kaggle/working/DM2024-Lab2-Master/pics/pic4.png')
# Answer here

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
### Interpretation of the Graphs and Their Relation to Overfitting/Underfitting

#### 1. **Left Plot: Model Accuracy**
- **Training Accuracy** (blue line):
  - Rapidly increases and approaches close to 1.0 (100%) as training progresses.
- **Validation Accuracy** (orange line):
  - Increases initially, then plateaus and slightly decreases after a few epochs.

**Interpretation**:
- The gap between training and validation accuracy indicates **overfitting**. The model performs well on the training data but struggles to generalize.

---

#### 2. **Right Plot: Model Loss**
- **Training Loss** (blue line):
  - Consistently decreases, showing effective error minimization on the training data.
- **Validation Loss** (orange line):
  - Decreases initially but diverges and increases after several epochs.

**Interpretation**:
- Increasing validation loss while training loss decreases is a sign of **overfitting**. The model starts to memorize the training data instead of learning general patterns.

---

#### 3. **Relation to Overfitting/Underfitting**
- **Overfitting**:
  - Observed when:
    - Training accuracy is high, but validation accuracy stagnates or decreases.
    - Training loss is low, but validation loss increases.
  - Overfitting occurs when the model becomes too complex and focuses too much on the training data.

- **Underfitting**:
  - If both training and validation accuracy remained low or loss values stayed high, it would indicate **underfitting**. This occurs when the model is too simdd(Dropout(0.5))  # Example: Adding dropout

---
## 7. Word2Vector

We will introduce how to use `gensim` to train your word2vec model and how to load a pre-trained model.

https://radimrehurek.com/gensim/index.html
!pip install nltk
import nltk

# Download the 'punkt_tab' data package
nltk.download('punkt_tab')

## check library
import gensim

## ignore warnings
import warnings
warnings.filterwarnings('ignore')

# # if you want to see the training messages, you can use it
# import logging
# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

## the input type
train_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))
train_df[['id', 'text', 'text_tokenized']].head()
## create the training corpus
training_corpus = train_df['text_tokenized'].values
training_corpus[:3]
## 7.2 Training our model
You can try to train your own model. More details: https://radimrehurek.com/gensim/models/word2vec.html
from gensim.models import Word2Vec

## setting
vector_dim = 100
window_size = 5
min_count = 1
training_epochs = 20

## model
word2vec_model = Word2Vec(sentences=training_corpus,
                          vector_size=vector_dim, window=window_size,
                          min_count=min_count, epochs=training_epochs)
### 7.3 Generating word vector (embeddings)
# get the corresponding vector of a word
word_vec = word2vec_model.wv['happy']
word_vec
# Get the most similar words
word = 'happy'
topn = 10
word2vec_model.wv.most_similar(word, topn=topn)
### 7.4 Using a pre-trained w2v model

Instead of training your own model ,you can use a model that has already been trained. Here, we see 2 ways of doing that:


#### (1) Download model by yourself

source: [GoogleNews-vectors-negative300](https://code.google.com/archive/p/word2vec/)

more details: https://radimrehurek.com/gensim/models/keyedvectors.html
!ls -al /kaggle/working/DM2024-Lab2-Master/GoogleNews/
from gensim.models import KeyedVectors

## Note: this model is very huge, this will take some time ...
model_path = "GoogleNews/GoogleNews-vectors-negative300.bin.gz"
w2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)
print('load ok')

w2v_google_model.most_similar('happy', topn=10)
#### (2) Using gensim api

Other pretrained models are available here: https://github.com/RaRe-Technologies/gensim-dataa
import gensim.downloader as api

## If you see `SSL: CERTIFICATE_VERIFY_FAILED` error, use this:
import ssl
import urllib.request
ssl._create_default_https_context = ssl._create_unverified_context

glove_twitter_25_model = api.load("glove-twitter-25")
print('load ok')

glove_twitter_25_model.most_similar('happy', topn=10)
## 7.5 king + woman - man = ?
Let's run one of the most famous examples for Word2Vec and compute the similarity between these 3 words:
w2v_google_model.most_similar(positive=['king', 'woman'], negative=['man'])
---
### ** >>> Exercise 7 (Take home): **  

Now, we have the word vectors, but our input data is a sequence of words (or say sentence).
How can we utilize these "word" vectors to represent the sentence data and train our model?

ç¾åœ¨ï¼Œæˆ‘å€‘æœ‰äº†å–®å­—å‘é‡ï¼Œä½†æˆ‘å€‘çš„è¼¸å…¥è³‡æ–™æ˜¯å–®å­—åºåˆ—ï¼ˆæˆ–å¥å­ï¼‰ã€‚
æˆ‘å€‘å¦‚ä½•åˆ©ç”¨é€™äº›ã€Œå–®å­—ã€å‘é‡ä¾†è¡¨ç¤ºå¥å­è³‡æ–™ä¸¦è¨“ç·´æˆ‘å€‘çš„æ¨¡åž‹ï¼Ÿ
### Answer here

è¦åˆ©ç”¨å–®å­—å‘é‡ä¾†è¡¨ç¤ºå¥å­ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹ç°¡å–®æ­¥é©Ÿé€²è¡Œï¼š

1. **å°‡å¥å­çš„æ¯å€‹å–®å­—è½‰æ›æˆå–®å­—å‘é‡**ï¼š
   - å°‡å¥å­ä¸­çš„æ¯å€‹å–®å­—è½‰æ›æˆå°æ‡‰çš„å–®å­—å‘é‡ï¼Œé€™äº›å‘é‡èƒ½å¤ æ•æ‰å–®å­—çš„æ„ç¾©ã€‚

2. **å°‡å–®å­—å‘é‡çµ„åˆç‚ºå¥å­çš„å‘é‡è¡¨ç¤º**ï¼š
   - å› ç‚ºå¥å­ç”±å¤šå€‹å–®å­—çµ„æˆï¼Œæˆ‘å€‘éœ€è¦ä¸€ç¨®æ–¹æ³•ä¾†å°‡å–®å­—å‘é‡çµåˆæˆä»£è¡¨æ•´å€‹å¥å­çš„å‘é‡ã€‚å¸¸è¦‹çš„æ–¹æ³•åŒ…æ‹¬ï¼š
     - **å¹³å‡æ³•**ï¼šè¨ˆç®—å¥å­ä¸­æ‰€æœ‰å–®å­—å‘é‡çš„å¹³å‡å€¼ï¼Œå¾—åˆ°ä¸€å€‹ä»£è¡¨å¥å­æ•´é«”æ„ç¾©çš„å‘é‡ã€‚
     - **åŠ ç¸½æ³•**ï¼šå°‡æ‰€æœ‰å–®å­—å‘é‡ç›¸åŠ ï¼ˆä¸å¹³å‡ï¼‰ï¼Œå¾—åˆ°å¥å­çš„å‘é‡è¡¨ç¤ºã€‚
     - **ä½¿ç”¨å¾ªç’°å±¤ï¼ˆRNNï¼‰**ï¼šå°‡å–®å­—å‘é‡åºåˆ—è¼¸å…¥åˆ° RNN æˆ– LSTM ä¸­ï¼Œç”±æ¨¡åž‹å­¸ç¿’å¥å­çš„ç¶œåˆè¡¨ç¤ºã€‚
     - **ä½¿ç”¨ Transformer æ¨¡åž‹**ï¼šåƒ BERT é€™æ¨£çš„é€²éšŽæ¨¡åž‹å¯ä»¥ç›´æŽ¥è™•ç†å–®å­—å‘é‡åºåˆ—ï¼Œä¸¦ç”¢ç”Ÿå¥å­çš„å‘é‡è¡¨ç¤ºã€‚

3. **è¨“ç·´æ¨¡åž‹**ï¼š
   - ä¸€æ—¦å¥å­è¢«è½‰æ›ç‚ºå–®ä¸€å‘é‡ï¼ˆæˆ–ç¶“éŽ RNN/Transformer æ¨¡åž‹è®Šæˆå¥å­çš„å‘é‡è¡¨ç¤ºï¼‰ï¼Œå°±å¯ä»¥å°‡å…¶ä½œç‚ºè¼¸å…¥ï¼Œé€²è¡Œæ©Ÿå™¨å­¸ç¿’æ¨¡åž‹çš„è¨“ç·´ã€‚

---

### ç°¡å–®çš„ä¾‹å­
å‡è¨­æˆ‘å€‘æœ‰ä¸€å€‹å¥å­ "I love cats"ï¼š
- å°‡ "I"ã€"love" å’Œ "cats" åˆ†åˆ¥è½‰æ›ç‚ºå–®å­—å‘é‡ï¼Œå¾—åˆ° `[v_I, v_love, v_cats]`ã€‚
- å°‡é€™äº›å‘é‡çµåˆï¼Œä¾‹å¦‚ç”¨å¹³å‡æ³•ï¼š`sentence_vector = (v_I + v_love + v_cats) / 3`ã€‚
- å°‡ `sentence_vector` ä½œç‚ºè¼¸å…¥ï¼Œç”¨æ–¼æ¨¡åž‹çš„è¨“ç·´ã€‚

é€™å€‹éŽç¨‹å¯ä»¥ç¢ºä¿å¥å­è¢«è½‰æ›æˆæ•¸å€¼è¡¨ç¤ºï¼Œä¸¦ä¸”å¯ä»¥è¢«æ©Ÿå™¨å­¸ç¿’æ¨¡åž‹è™•ç†ï¼

# 9. High-dimension Visualization: t-SNE and UMAP
No matter if you use the Bag-of-words, TF-IDF, or Word2Vec, it's very hard to see the embedding result, because the dimension is larger than 3.

In Lab 1, we already talked about PCA, t-SNE and UMAP. We can use PCA to reduce the dimension of our data, then visualize it. However, if you dig deeper into the result, you'd find it is insufficient.

Our aim will be to create a visualization similar to the one below with t-SNE:
# picture
from IPython.display import Image
Image('/kaggle/working/DM2024-Lab2-Master/pics/pic7.png')
source: https://www.fabian-keller.de/research/high-dimensional-data-visualization

And also like this for UMAP:
# picture
from IPython.display import Image
Image('/kaggle/working/DM2024-Lab2-Master/pics/pic9.png')
source: https://umap-learn.readthedocs.io/en/latest/auto_examples/plot_mnist_example.html

t-SNE and UMAP reference:
http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html https://umap-learn.readthedocs.io/en/latest/
## 9.1 Prepare visualizing target
Let's prepare data lists like:

happy words
angry words
data words
mining words
word_list = ['happy', 'angry', 'data', 'mining']

topn = 5
happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]
angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]
data_words = ['data'] + [word_ for word_, sim_ in w2v_google_model.most_similar('data', topn=topn)]
mining_words = ['mining'] + [word_ for word_, sim_ in w2v_google_model.most_similar('mining', topn=topn)]

print('happy_words: ', happy_words)
print('angry_words: ', angry_words)
print('data_words: ', data_words)
print('mining_words: ', mining_words)

target_words = happy_words + angry_words + data_words + mining_words
print('\ntarget words: ')
print(target_words)

print('\ncolor list:')
cn = topn + 1
color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn
print(color)
## 9.2 Plot using t-SNE and UMAP (2-dimension)

%matplotlib inline
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

## w2v model
model = w2v_google_model

## prepare training word vectors
size = 200
target_size = len(target_words)
all_word = list(model.index_to_key)
word_train = target_words + all_word[:size]
X_train = model[word_train]

## t-SNE model
tsne = TSNE(n_components=2, metric='cosine', random_state=28)

## training
X_tsne = tsne.fit_transform(X_train)

## plot the result
plt.figure(figsize=(7.5, 7.5), dpi=115)
plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)
for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):
    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')
plt.show()
!pip install umap-learn
import matplotlib.pyplot as plt
import umap.umap_ as umap

## w2v model
model = w2v_google_model

## prepare training word vectors
size = 200
target_size = len(target_words)
all_word = list(model.index_to_key)
word_train = target_words + all_word[:size]
X_train = model[word_train]

## UMAP model
umap_model = umap.UMAP(n_components=2, metric='cosine', random_state=28)

## training
X_umap = umap_model.fit_transform(X_train)

## plot the result
plt.figure(figsize=(7.5, 7.5), dpi=115)
plt.scatter(X_umap[:target_size, 0], X_umap[:target_size, 1], c=color)
for label, x, y in zip(target_words, X_umap[:target_size, 0], X_umap[:target_size, 1]):
    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')
plt.show()
## ** >>> Exercise 8 (Take home): **
Generate a t-SNE and UMAP visualization to show the 15 words most related to the words "angry", "happy", "sad", "fear" (60 words total). Compare the differences between both graphs.
# Answer here

# 1. Prepare the data:

import nltk
import numpy as np
from sklearn.manifold import TSNE
import umap
import matplotlib.pyplot as plt
from gensim.models import KeyedVectors

# Load the pre-trained Word2Vec model:
model_path = "GoogleNews/GoogleNews-vectors-negative300.bin.gz"  # Update with the actual path if different
w2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)

# Define the target words:
target_words = ["angry", "happy", "sad", "fear"]

# Get the most similar words for each target word:
topn = 15  # Number of similar words to retrieve

similar_words_dict = {}
for word in target_words:
    similar_words_dict[word] = [word] + [
        word_ for word_, sim_ in w2v_google_model.most_similar(word, topn=topn)
    ]

# Combine all similar words into a single list:
all_similar_words = [
    word for sublist in similar_words_dict.values() for word in sublist
]

# Get word vectors for all similar words:
word_vectors = [w2v_google_model[word] for word in all_similar_words]

print('ok')
# [Answer]
# 2. Generate the t-SNE visualization:

# Import necessary libraries
import nltk
import numpy as np
from sklearn.manifold import TSNE
import umap
import matplotlib.pyplot as plt
from gensim.models import KeyedVectors

# Load the pre-trained Word2Vec model (if not already loaded)
model_path = "GoogleNews/GoogleNews-vectors-negative300.bin.gz"
w2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)

# Define the target words
target_words = ["angry", "happy", "sad", "fear"]

# Get the most similar words for each target word
topn = 15  # Number of similar words to retrieve

similar_words_dict = {}
for word in target_words:
    similar_words_dict[word] = [word] + [
        word_ for word_, sim_ in w2v_google_model.most_similar(word, topn=topn)
    ]

# Combine all similar words into a single list
all_similar_words = [
    word for sublist in similar_words_dict.values() for word in sublist
]

# Get word vectors for all similar words
word_vectors = [w2v_google_model[word] for word in all_similar_words]

# Apply t-SNE
tsne = TSNE(n_components=2, random_state=42)
# Convert word_vectors to a NumPy array
word_vectors_np = np.array(word_vectors)  # convert list of arrays to a single array
tsne_embeddings = tsne.fit_transform(word_vectors_np)  # pass the numpy array

# Plot the t-SNE embeddings
plt.figure(figsize=(10, 8))
for i, word in enumerate(all_similar_words):
    plt.scatter(tsne_embeddings[i, 0], tsne_embeddings[i, 1])
    plt.annotate(
        word,
        xy=(tsne_embeddings[i, 0], tsne_embeddings[i, 1]),
        xytext=(5, 2),
        textcoords="offset points",
        ha="right",
        va="bottom",
    )
plt.title("t-SNE Visualization of Similar Words")
plt.show()
# [Answer]
# 3. Generate the UMAP visualization:
# Apply UMAP:
umap_reducer = umap.UMAP(random_state=42)
umap_embeddings = umap_reducer.fit_transform(word_vectors)

# Plot the UMAP embeddings:
plt.figure(figsize=(10, 8))
for i, word in enumerate(all_similar_words):
    plt.scatter(umap_embeddings[i, 0], umap_embeddings[i, 1])
    plt.annotate(
        word,
        xy=(umap_embeddings[i, 0], umap_embeddings[i, 1]),
        xytext=(5, 2),
        textcoords="offset points",
        ha="right",
        va="bottom",
    )
plt.title("UMAP Visualization of Similar Words")
plt.show()
#### [Answer]
4. Comparison of t-SNE and UMAP visualizations:

### Comparison of t-SNE and UMAP Visualizations

1. **Global Structure**:
   - UMAP generally preserves the global structure of the data better than t-SNE.
   - Clusters of similar words are more likely to be grouped together in the UMAP visualization.

2. **Local Structure**:
   - t-SNE often does a better job of preserving local structure.
   - It may show finer details within clusters, revealing more nuanced relationships.

3. **Computational Efficiency**:
   - UMAP is typically faster than t-SNE, especially for large datasets.

4. **Interpretability**:
   - Both t-SNE and UMAP are nonlinear dimensionality reduction techniques, making the resulting visualizations difficult to interpret definitively.
   - However, general patterns and clusters of similar words can often be observed.

5. **Application to Emotion Categories**:
   - In this specific case:
     - **UMAP**: Likely provides a clearer separation between different emotion categories.
     - **t-SNE**: May reveal more nuanced relationships within each emotion cluster.

6. **Conclusion**:
   - The choice between t-SNE and UMAP depends on:
     - The specific goals of the visualization (e.g., global vs. local patterns).
     - The characteristics of the data (e.g., dataset size and complexity).

# 10. Large Language Models (LLMs)
Large Language Models (LLMs) are AI models trained on vast text data to understand and generate human language. Models like GPT and BERT excel at tasks like translation, summarization, and sentiment analysis due to their deep learning techniques and large-scale training. Recently these models got popular with the rise of Open-AI's ChatGPT and their different models, showcasing the potential of these models in a lot of aspects of our current society.

Open-source LLMs are cost-effective and customizable, with strong community support, but may underperform compared to paid models and require technical expertise to manage. Paid LLMs offer superior performance, ease of use, and regular updates, but are costly, less flexible, and create dependency on external providers for ongoing access and updates.

## Open Source LLMs:
In this lab we are going to use Ollama (Ollama GitHub Link), that is a library that let us use a long list of open-source LLMs of differing size. For this section we are going to be using 'llama3.2' or 'llama3.2:1b' for text based tasks, and 'llava-phi3' for multi-modal tasks (e.g. image to text). Ollama has a great variety of models, and those can be found here: model library. You are free to explore them if you want to try using them, you can check the advantages and disadvantages of each.

Or they can also be observed in here:
## Ollama ç’°å¢ƒå‰ç½®ä½œæ¥­
0. kaggle ç’°å¢ƒå¾žé€™ä¸€æ®µé–‹å§‹é–‹ GPU
1. ä¸‹è¼‰ Ollama
2. è¦åŸ·è¡Œ Ollama
3. ä¸‹è¼‰æ¨¡åž‹
#Download ollama
!curl -fsSL https://ollama.com/install.sh | sh

#Run ollama
import subprocess
# å•Ÿå‹•æœå‹™ä¸¦éš±è—æ‰€æœ‰è¼¸å‡º
process = subprocess.Popen("ollama serve > /dev/null 2>&1", shell=True)
print("Ollama is running")

#Download model
!ollama pull llama3.2 > /dev/null 2>&1
print("model pulled")
from IPython.display import display, Markdown, Latex
display(Markdown('*some markdown* $\phi$'))
display(Markdown(response['message']['content']))
## ** >>> Exercise 9 (Take home): **
You noticed there is a role parameter inside the ollama.chat function, investigate what other roles there can be inside the function and what do they do. Give an example of a prompt using another role in additional to the user one.
# prompt: You noticed there is a role parameter inside the ollama.chat function, investigate what other roles there can be inside the function and what do they do. Give an example of a prompt using another role in additional to the user one.

import ollama
from IPython.display import Markdown

response = ollama.chat(model='llama3.2', messages=[
    {'role': 'system', 'content': 'You are a helpful assistant that translates English to Japanese.'},
    {'role': 'user', 'content': 'Translate "Hello, how are you?" to Japanese.'}
])

print(response['message']['content'])
display(Markdown(response['message']['content']))
## 10.2 Multi-Modal Prompting - Text + Images
Multi-modal prompting involves using input from multiple sources or modes, such as text, images, or audio, to guide a model's response. It allows AI to process and generate information based on more than one type of input.

For image plus text prompting, the model receives both an image and a related text prompt. The image provides visual context, while the text gives additional guidance. The model uses both inputs to generate more accurate and contextually relevant responses, which is useful for tasks like image captioning, visual question answering, or content generation based on visual cues.

Let's look at the following images that are in the pics folder in the directory of this notebook:
# picture
from IPython.display import Image
Image('pics/example1.png')
source: https://cooljapan-videos.com/tw/articles/epe0y86g
# picture
from IPython.display import Image
Image('pics/example2.jpg')
source: https://www.istockphoto.com/photo/young-cat-scottish-straight-gm1098182434-294927481
## We will use the llava-phi3 model that we installed to request a description of the images
!ollama pull llava-phi3
# åˆ¤è®€ç‹—ç‹—çš„ç…§ç‰‡
import ollama
from IPython.display import Markdown, display # Import Markdown from IPython.display

response2 = ollama.chat(model='llava-phi3', messages=[
    {
        'role': 'user',
        'content': 'What is this image about? ',
        'images': ['pics/example1.png'] #Image with the dog
    },
])

display(Markdown(response2['message']['content']))
## ** >>> Exercise 10 (Take home): **
Try asking the model with one image of your choosing. Is the description accurate? Why?
# picture
from IPython.display import Image
Image('pics/monkey.jpg')
# Import necessary libraries
import ollama
from IPython.display import Markdown, display  # For displaying Markdown

# Step 1: Use Ollama to describe the image
response4 = ollama.chat(model='llava-phi3', messages=[
    {
        'role': 'user',
        'content': 'What is this image about?',
        'images': ['pics/monkey.jpg']  # Image with the monkey
    },
])

# Display the response for debugging or visualization
display(Markdown(response4['message']['content']))


## 10.3 Retrieval-Augmented Generation (RAG)
RAG (Retrieval-Augmented Generation) is a technique where a language model combines document retrieval with text generation. In RAG, a retrieval system first finds relevant documents or text chunks, and then the language model uses this retrieved information to generate a more informed and accurate response. This method enhances the model's ability to answer questions by grounding its responses in real, external data.

In the following code, we will load a webpage as a document, which allows us to retrieve text from a URL. After loading the content, we will split the document into smaller, manageable chunks, making it easier for our model to process. Then, we'll generate embeddings for these chunks with a specified LLM model (e.g., Llama3.2). These embeddings will be stored in a vector database, which enables us to perform similarity searches. By setting up this retrieval system, we can use a RAG chain to answer questions. The retriever finds relevant text chunks from the document based on a query, and the LLM generates a response by incorporating this retrieved information, making the answers more grounded and accurate.

RAGï¼ˆæª¢ç´¢å¢žå¼·ç”Ÿæˆï¼‰æ˜¯ä¸€ç¨®èªžè¨€æ¨¡åž‹å°‡æ–‡ä»¶æª¢ç´¢èˆ‡æ–‡å­—ç”Ÿæˆçµåˆçš„æŠ€è¡“ã€‚åœ¨ RAG ä¸­ï¼Œæª¢ç´¢ç³»çµ±é¦–å…ˆå°‹æ‰¾ç›¸é—œæ–‡ä»¶æˆ–æ–‡å­—å€å¡Šï¼Œç„¶å¾Œèªžè¨€æ¨¡åž‹ä½¿ç”¨æª¢ç´¢åˆ°çš„è³‡è¨Šç”¢ç”Ÿæ›´æ˜Žæ™ºã€æ›´æº–ç¢ºçš„å›žæ‡‰ã€‚è©²æ–¹æ³•é€éŽå°‡æ¨¡åž‹çš„éŸ¿æ‡‰åŸºæ–¼çœŸå¯¦çš„å¤–éƒ¨æ•¸æ“šä¾†å¢žå¼·æ¨¡åž‹å›žç­”å•é¡Œçš„èƒ½åŠ›ã€‚

åœ¨ä¸‹é¢çš„ç¨‹å¼ç¢¼ä¸­ï¼Œæˆ‘å€‘å°‡è¼‰å…¥ä¸€å€‹ç¶²é ä½œç‚ºæ–‡æª”ï¼Œé€™å…è¨±æˆ‘å€‘å¾ž URL æª¢ç´¢æ–‡å­—ã€‚è¼‰å…¥å…§å®¹å¾Œï¼Œæˆ‘å€‘å°‡æŠŠæ–‡ä»¶åˆ†å‰²æˆæ›´å°çš„ã€å¯ç®¡ç†çš„å€å¡Šï¼Œä½¿æˆ‘å€‘çš„æ¨¡åž‹æ›´å®¹æ˜“è™•ç†ã€‚ç„¶å¾Œï¼Œæˆ‘å€‘å°‡ä½¿ç”¨æŒ‡å®šçš„ LLM æ¨¡åž‹ï¼ˆä¾‹å¦‚ Llama3.2ï¼‰ç‚ºé€™äº›å€å¡Šç”¢ç”ŸåµŒå…¥ã€‚é€™äº›åµŒå…¥å°‡å„²å­˜åœ¨å‘é‡è³‡æ–™åº«ä¸­ï¼Œé€™ä½¿æˆ‘å€‘èƒ½å¤ åŸ·è¡Œç›¸ä¼¼æ€§æœå°‹ã€‚é€éŽå»ºç«‹é€™å€‹æª¢ç´¢ç³»çµ±ï¼Œæˆ‘å€‘å¯ä»¥ä½¿ç”¨RAGéˆä¾†å›žç­”å•é¡Œã€‚æª¢ç´¢å™¨æ ¹æ“šæŸ¥è©¢å¾žæ–‡ä»¶ä¸­å°‹æ‰¾ç›¸é—œæ–‡å­—å€å¡Šï¼Œæ³•å­¸ç¢©å£«é€éŽåˆä½µé€™äº›æª¢ç´¢åˆ°çš„è³‡è¨Šä¾†ç”¢ç”Ÿå›žæ‡‰ï¼Œä½¿ç­”æ¡ˆæ›´åŠ åŸºç¤Žå’Œæº–ç¢ºã€‚
!ollama pull llama3.2
!pip install ollama
!pip install langchain_community
!pip install chromadb
!pip install -U langchain-ollama
!ollama pull llama3.2
# RAGä¹‹å‰è¦è·‘ 20 åˆ†é˜
# ç¾åœ¨æœ‰GPUè·‘ 8.45 ç§’

import ollama
import bs4
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_ollama import OllamaEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from IPython.display import Markdown, display

import os
os.environ["USER_AGENT"] = "Lab2-RAG-colab-notebook"  # æˆ–å…¶ä»–æè¿°æ€§åç¨±

embeddings = OllamaEmbeddings(model="llama3.2")

llm_model = "llama3.2" #You can change to the one of your preference

# Function to load, split, and retrieve documents
def load_and_retrieve_docs(url):
    loader = WebBaseLoader(
        web_paths=(url,),
        bs_kwargs=dict()
    )
    docs = loader.load() #We will load the URL that will serve as our data source
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) #We will divide the URL in chunks of text for easier comparison in the vector space
    splits = text_splitter.split_documents(docs)
    #print(splits) #You can print this to see how the chunks in the url where split
    embeddings = OllamaEmbeddings(model=llm_model) #Generating embeddings with our chosen model
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings) #Our vector space for comparison
    return vectorstore.as_retriever()

url="https://www.ibm.com/topics/large-language-models"
# Create the retriever
retriever = load_and_retrieve_docs(url)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs) #Format the retrieved docs in an orderly manner for prompting

# Define the Ollama LLM function
def ollama_llm(question, context):
    formatted_prompt = f"Question: {question}\n\nContext: {context}"
    response = ollama.chat(model='llama3.2', messages=[{'role': 'user', 'content': formatted_prompt}])
    return response['message']['content']

# Define the RAG chain
def rag_chain(question):
    retrieved_docs = retriever.invoke(question)
    formatted_context = format_docs(retrieved_docs)
    return ollama_llm(question, formatted_context)

# Use the RAG chain
result = rag_chain("What are the related solutions of IBM with LLMs?")
display(Markdown(result))
# picture
from IPython.display import Image
Image('pics/pic11.png')
source: https://www.ibm.com/topics/large-language-models
## ** >>> Exercise 11 (Take home): **
Try to modify the code to make it accept three URLs, or three text documents of your choosing. After modifying it, make three prompts/questions with information that can be found in each of the documents/urls, compare the accuracy of the response with the actual answer. Investigate and discuss the advantages and disadvantages of RAG systems.
!pip install ollama
!pip install langchain_community
!pip install chromadb
# Answer here
# ä¹‹å‰åŸ·è¡Œæ™‚é–“ : 58 åˆ†é˜
# æœ‰GPU åªè¦  30 ç§’

import ollama
import bs4
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from IPython.display import Markdown, display

llm_model = "llama3.2"  # You can change to the one of your preference

# Function to load, split, and retrieve documents from multiple URLs
def load_and_retrieve_docs(urls):
    all_splits = []
    for url in urls:
        loader = WebBaseLoader(
            web_paths=(url,),
            bs_kwargs=dict()
        )
        docs = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
        all_splits.extend(splits)

    embeddings = OllamaEmbeddings(model=llm_model)
    vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings)
    return vectorstore.as_retriever()

# Provide multiple URLs
urls = [
    "https://www.ibm.com/topics/large-language-models",
    "https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/",
    "https://www.oracle.com/ng/artificial-intelligence/machine-learning/what-is-machine-learning/"
]
# Create the retriever
retriever = load_and_retrieve_docs(urls)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Define the Ollama LLM function
def ollama_llm(question, context):
    formatted_prompt = f"Question: {question}\n\nContext: {context}"
    response = ollama.chat(model='llama3.2', messages=[{'role': 'user', 'content': formatted_prompt}])
    return response['message']['content']

# Define the RAG chain
def rag_chain(question):
    retrieved_docs = retriever.invoke(question)
    formatted_context = format_docs(retrieved_docs)
    return ollama_llm(question, formatted_context)

# Test the RAG system with three questions
questions = [
    "What are IBM's solutions related to large language models?",
    "What Is Retrieval-Augmented Generation (RAG) on Oracle",
    "What Is Machine Learning on Oracle ?"
]

# Evaluate the RAG system
for question in questions:
    result = rag_chain(question)
    display(Markdown(f"### Question: {question}\n{result}"))

## 10.4 Generating LLM Embeddings:
LLM embeddings are dense vector representations of text generated by Large Language Models. These embeddings, like we have already seen in the lab, capture the semantic meaning and relationships between words, phrases, or even entire documents by mapping them into a high-dimensional space where similar pieces of text are placed closer together. What makes LLM embeddings special is that they are contextual and rich in meaning, meaning the same word can have different embeddings based on its surrounding context.

For example, the word "bank" would have different embeddings in the sentences "I sat by the river bank" and "I deposited money in the bank." This ability to understand and encode context enables LLM embeddings to outperform traditional techniques (like TF-IDF or one-hot encoding) by providing a deeper, more nuanced representation of language.

Additionally, LLM embeddings are pre-trained on vast amounts of data, allowing them to generalize well across different tasks (like classification, clustering, or similarity detection) without the need for extensive retraining. This makes them highly valuable in many natural language processing tasks today.

## 10.4 ç”¢ç”Ÿ LLM åµŒå…¥ï¼š
LLM åµŒå…¥æ˜¯å¤§åž‹èªžè¨€æ¨¡åž‹ç”¢ç”Ÿçš„æ–‡æœ¬çš„å¯†é›†å‘é‡è¡¨ç¤ºã€‚å°±åƒæˆ‘å€‘åœ¨å¯¦é©—å®¤ä¸­çœ‹åˆ°çš„é‚£æ¨£ï¼Œé€™äº›åµŒå…¥é€éŽå°‡å–®å­—ã€çŸ­èªžç”šè‡³æ•´å€‹æ–‡ä»¶æ˜ å°„åˆ°é«˜ç¶­ç©ºé–“ï¼ˆå…¶ä¸­ç›¸ä¼¼çš„æ–‡æœ¬ç‰‡æ®µæ›´ç·Šå¯†åœ°æ”¾ç½®åœ¨ä¸€èµ·ï¼‰ä¾†æ•ç²å–®å­—ã€çŸ­èªžç”šè‡³æ•´å€‹æ–‡ä»¶ä¹‹é–“çš„èªžç¾©å’Œé—œä¿‚ã€‚ LLM åµŒå…¥çš„ç‰¹æ®Šä¹‹è™•åœ¨æ–¼å®ƒå€‘èˆ‡ä¸Šä¸‹æ–‡ç›¸é—œä¸”å«ç¾©è±å¯Œï¼Œé€™æ„å‘³è‘—åŒä¸€å€‹å–®å­—å¯ä»¥æ ¹æ“šå…¶å‘¨åœçš„ä¸Šä¸‹æ–‡æœ‰ä¸åŒçš„åµŒå…¥ã€‚

ä¾‹å¦‚ï¼Œã€ŒéŠ€è¡Œã€ä¸€è©žåœ¨ã€Œæˆ‘ååœ¨æ²³å²¸é‚Šã€å’Œã€Œæˆ‘æŠŠéŒ¢å­˜å…¥éŠ€è¡Œã€é€™å…©å€‹å¥å­ä¸­æœƒæœ‰ä¸åŒçš„åµŒå…¥ã€‚é€™ç¨®ç†è§£å’Œç·¨ç¢¼ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ä½¿ LLM åµŒå…¥èƒ½å¤ æä¾›æ›´æ·±å…¥ã€æ›´ç´°ç·»çš„èªžè¨€è¡¨ç¤ºï¼Œå¾žè€Œè¶…è¶Šå‚³çµ±æŠ€è¡“ï¼ˆå¦‚ TF-IDF æˆ– one-hot ç·¨ç¢¼ï¼‰ã€‚

æ­¤å¤–ï¼ŒLLM åµŒå…¥æ˜¯åœ¨å¤§é‡è³‡æ–™ä¸Šé€²è¡Œé è¨“ç·´çš„ï¼Œä½¿å®ƒå€‘èƒ½å¤ åœ¨ä¸åŒçš„ä»»å‹™ï¼ˆå¦‚åˆ†é¡žã€èšé¡žæˆ–ç›¸ä¼¼æ€§æª¢æ¸¬ï¼‰ä¸­å¾ˆå¥½åœ°æ³›åŒ–ï¼Œè€Œç„¡éœ€é€²è¡Œå¤§é‡çš„å†è¨“ç·´ã€‚é€™ä½¿å¾—å®ƒå€‘åœ¨ç•¶ä»Šçš„è¨±å¤šè‡ªç„¶èªžè¨€è™•ç†ä»»å‹™ä¸­éžå¸¸æœ‰åƒ¹å€¼ã€‚

ç¾åœ¨è®“æˆ‘å€‘ä½¿ç”¨ llama 3.2 ç‚ºæˆ‘å€‘çš„è³‡æ–™é›†ç”¢ç”Ÿä¸€äº›åµŒå…¥ï¼š


Now let's generate some embeddings with llama 3.2 for our dataset:
!ollama pull llama3.2
!pip install ollama

!pip install pandas
import pandas as pd

### training data
anger_train = pd.read_csv("data/semeval/train/anger-ratings-0to1.train.txt",
                         sep="\t", header=None,names=["id", "text", "emotion", "intensity"])
sadness_train = pd.read_csv("data/semeval/train/sadness-ratings-0to1.train.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
fear_train = pd.read_csv("data/semeval/train/fear-ratings-0to1.train.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
joy_train = pd.read_csv("data/semeval/train/joy-ratings-0to1.train.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])

train_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)

### testing data
anger_test = pd.read_csv("data/semeval/dev/anger-ratings-0to1.dev.gold.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
sadness_test = pd.read_csv("data/semeval/dev/sadness-ratings-0to1.dev.gold.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
fear_test = pd.read_csv("data/semeval/dev/fear-ratings-0to1.dev.gold.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])
joy_test = pd.read_csv("data/semeval/dev/joy-ratings-0to1.dev.gold.txt",
                         sep="\t", header=None, names=["id", "text", "emotion", "intensity"])



# combine 4 sub-dataset
test_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)
train_df.head()						 

# shuffle dataset éš¨æ©Ÿæ‰“äº‚ï¼ˆshuffleï¼‰è³‡æ–™é›†çš„é †åº
train_df = train_df.sample(frac=1)
test_df = test_df.sample(frac=1)
print('ok')
# LlamaEmbeddingGenerator

# ä¹‹å‰è·‘ 8 hrs
# æœ‰GPU è·‘ 2 åˆ† 42 ç§’

import pandas as pd
import ollama
import os

# Let's copy our dataframes for training and testing
df_train = train_df  # Assuming train_df is defined elsewhere
df_test = test_df  # Assuming test_df is defined elsewhere

# Define a function to generate embeddings
def generate_embeddings(row, text_column_name='text'):
    # Access the row's text using the attribute name, not the string
    embeddings = ollama.embeddings(
        model='llama3.2',
        prompt=getattr(row, text_column_name),  # Use getattr to access attribute by name
    )
    return embeddings["embedding"]

# We use the text column
column_name = 'text'

# Apply the function to the specified column and store the result in a new 'embeddings' column
# Use list comprehension for performance

df_train['embeddings'] = [generate_embeddings(row, column_name) for row in df_train.itertuples(index=False)]
# Apply the function to the specified column and store the result in a new 'embeddings' column


df_test['embeddings'] = [generate_embeddings(row, column_name) for row in df_test.itertuples(index=False)]


df_train #We can see the new column with the embeddings
df_test #We can see the new column with the embeddings
## Now let's train some models with these embeddings:

KNeighborsClassifier (KNN): KNN is a simple, instance-based machine learning algorithm used for classification. It works by finding the 'k' nearest neighbors to a data point based on a distance metric (e.g., Euclidean distance) and assigning the most common class among those neighbors to the data point. KNN is non-parametric, meaning it doesnâ€™t assume a specific form for the underlying data distribution, and it classifies points based on their similarity to other points in the training set. It's easy to understand and implement, though it can become computationally expensive with large datasets.

## ç¾åœ¨è®“æˆ‘å€‘ç”¨é€™äº›åµŒå…¥ä¾†è¨“ç·´ä¸€äº›æ¨¡åž‹ï¼š

KNeighborsClassifier (KNN)ï¼šKNN æ˜¯ä¸€ç¨®ç°¡å–®çš„ã€åŸºæ–¼å¯¦ä¾‹çš„æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•ï¼Œç”¨æ–¼åˆ†é¡žã€‚å®ƒçš„å·¥ä½œåŽŸç†æ˜¯æ ¹æ“šè·é›¢æ¸¬é‡ï¼ˆä¾‹å¦‚æ­å¹¾é‡Œå¾·è·é›¢ï¼‰æ‰¾åˆ°æ•¸æ“šé»žçš„â€œkâ€å€‹æœ€è¿‘é„°å±…ï¼Œä¸¦å°‡é€™äº›é„°å±…ä¸­æœ€å¸¸è¦‹çš„é¡žåˆ¥åˆ†é…çµ¦æ•¸æ“šé»žã€‚ KNN æ˜¯éžåƒæ•¸çš„ï¼Œé€™æ„å‘³è‘—å®ƒä¸æœƒå‡è¨­åŸºç¤Žè³‡æ–™åˆ†ä½ˆçš„ç‰¹å®šå½¢å¼ï¼Œä¸¦ä¸”å®ƒæ ¹æ“šé»žèˆ‡è¨“ç·´é›†ä¸­å…¶ä»–é»žçš„ç›¸ä¼¼æ€§å°é»žé€²è¡Œåˆ†é¡žã€‚å®ƒå¾ˆå®¹æ˜“ç†è§£å’Œå¯¦ç¾ï¼Œä½†å°æ–¼å¤§åž‹è³‡æ–™é›†ä¾†èªªï¼Œè¨ˆç®—æˆæœ¬å¯èƒ½æœƒå¾ˆé«˜ã€‚
# Apply the function to the specified column and store the result in a new 'embeddings' column
df_test['embeddings'] = [generate_embeddings(row, column_name) for row in df_test.itertuples(index=False)]
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Let's use the new Llama 3.2 embeddings as our training features and the emotions as our labels
X_train = df_train["embeddings"].tolist()
y_train = df_train['emotion']


# Initialize the KNN classifier
knn = KNeighborsClassifier(n_neighbors=10)

knn.fit(X_train, y_train)
len(df_train["embeddings"][0]) # Llama 3.2 embedding dimension is 3072
X_test = df_test["embeddings"].tolist()
y_test = df_test['emotion']

# Predicting the label for the test data
y_pred = knn.predict(X_test)

#Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)

print(f'KNN Accuracy: {accuracy}')
## precision, recall, f1-score,
from sklearn.metrics import classification_report

print(classification_report(y_true=y_test, y_pred=y_pred))
## check by confusion matrix

# åŒ¯å…¥å¿…è¦çš„å¥—ä»¶
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# å®šç¾©æ··æ·†çŸ©é™£ç¹ªè£½å‡½æ•¸
def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):
    """
    ç¹ªè£½æ··æ·†çŸ©é™£çš„å‡½æ•¸
    :param cm: æ··æ·†çŸ©é™£
    :param classes: é¡žåˆ¥æ¨™ç±¤
    :param title: åœ–è¡¨æ¨™é¡Œ
    :param cmap: é…è‰²
    """
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap=cmap, xticklabels=classes, yticklabels=classes)
    plt.title(title)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

# æ··æ·†çŸ©é™£è¨ˆç®—
cm2 = confusion_matrix(y_true=y_test, y_pred=y_pred)
my_tags = ['anger', 'fear', 'joy', 'sadness']

# ç¹ªè£½æ··æ·†çŸ©é™£
plot_confusion_matrix(cm2, classes=my_tags, title='Confusion matrix for classification with \nLLM Embeddings - KNN')

## Now let's also try to apply our Neural Network to these embeddings:
## è¨»æ˜Ž : ä»¥ä¸‹ç‚ºå…¨éƒ¨æ•´åˆéŽçš„ç¨‹å¼ç¢¼
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tensorflow.keras.utils import to_categorical
from keras.models import Model
from keras.layers import Input, Dense, ReLU, Softmax
from keras.optimizers import Adam
from keras.callbacks import CSVLogger
import matplotlib.pyplot as plt

# å®šç¾©è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™
X_llama_train = np.vstack(df_train["embeddings"].to_numpy())  # è¨“ç·´é›†
X_llama_test = np.vstack(df_test["embeddings"].to_numpy())    # æ¸¬è©¦é›†

# åˆå§‹åŒ– LabelEncoder ä¸¦æ“¬åˆ
label_encoder = LabelEncoder()
label_encoder.fit(y_train)  # ä½¿ç”¨è¨“ç·´é›†æ¨™ç±¤ä¾†æ“¬åˆ

# One-hot ç·¨ç¢¼æ¨™ç±¤
y_train_llama = to_categorical(label_encoder.transform(y_train))
y_test_llama = to_categorical(label_encoder.transform(y_test))

# I/O æª¢æŸ¥
input_shape_llama = len(X_llama_train[0])
print('input_shape: ', input_shape_llama)
output_shape = len(label_encoder.classes_)
print('output_shape: ', output_shape)

# Neural Network æ¨¡åž‹å»ºæ§‹
model_input = Input(shape=(input_shape_llama,))
X = model_input
X_W1 = Dense(units=64)(X)
H1 = ReLU()(X_W1)
H1_W2 = Dense(units=64)(H1)
H2 = ReLU()(H1_W2)
H2_W3 = Dense(units=output_shape, activation='softmax')(H2)
model_output = H2_W3
model = Model(inputs=[model_input], outputs=[model_output])

# ç·¨è­¯æ¨¡åž‹
model.compile(optimizer=Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# é¡¯ç¤ºæ¨¡åž‹çµæ§‹
model.summary()

# è¨“ç·´è¨­å®š
csv_logger = CSVLogger('logs/training_log_2.csv', append=True)
epochs = 25
batch_size = 32

# é–‹å§‹è¨“ç·´
history = model.fit(X_llama_train, y_train_llama,
                    epochs=epochs,
                    batch_size=batch_size,
                    callbacks=[csv_logger],
                    validation_data=(X_llama_test, y_test_llama),
                    verbose=1)

# é æ¸¬
pred_proba = model.predict(X_llama_test, batch_size=128)
pred_result = label_encoder.inverse_transform(np.argmax(pred_proba, axis=1))

print('ok')
print('---')




## ** >>> Exercise 12 (Take home): **
Follow Exercise 6 again and Plot the Training and Validation Accuracy and Loss for the results of this Neural Network. Compare the results of both KNN and the NN we just implemented. Discuss about why we obtained these results with the LLM Embeddings compared to the results of the other models implemented in this Lab.

å†æ¬¡ä¾ç…§ç·´ç¿’ 6 ç¹ªè£½æ­¤ç¥žç¶“ç¶²è·¯çµæžœçš„è¨“ç·´å’Œé©—è­‰æº–ç¢ºæ€§å’Œæå¤±ã€‚æ¯”è¼ƒ KNN å’Œæˆ‘å€‘å‰›å‰›å¯¦ç¾çš„ NN çš„çµæžœã€‚èˆ‡æœ¬å¯¦é©—å®¤ä¸­å¯¦ç¾çš„å…¶ä»–æ¨¡åž‹çš„çµæžœç›¸æ¯”ï¼Œè¨Žè«–ç‚ºä»€éº¼æˆ‘å€‘ä½¿ç”¨ LLM åµŒå…¥ä¾†ç²å¾—é€™äº›çµæžœã€‚
# Answer :

# è¨“ç·´éŽç¨‹åˆ†æžï¼ˆExercise 6ï¼‰
## å¾žæº–ç¢ºçŽ‡æ›²ç·šä¾†çœ‹

- **è¨“ç·´é›†æº–ç¢ºçŽ‡**  
  éš¨è‘—è¨“ç·´è¼ªæ•¸ä¸æ–·ä¸Šå‡ï¼Œé”åˆ°äº†ç´„ 92% çš„æ°´å¹³ã€‚è€Œé©—è­‰é›†æº–ç¢ºçŽ‡ä¹Ÿåœ¨ä¸æ–·æå‡ï¼Œä½†æœ€çµ‚åªé”åˆ°äº† 70% å·¦å³ã€‚é€™è¡¨ç¤ºæ¨¡åž‹åœ¨è¨“ç·´è³‡æ–™ä¸Šå­¸ç¿’å¾—å¾ˆå¥½ï¼Œä½†åœ¨æ³›åŒ–åˆ°é©—è­‰é›†ä¸Šæ™‚é‚„å­˜åœ¨ä¸€å®šéŽæ“¬åˆçš„å•é¡Œã€‚

- **æå¤±å‡½æ•¸æ›²ç·š**  
  è¨“ç·´é›†æå¤±ä¸‹é™å¾ˆå¿«ï¼Œè€Œé©—è­‰é›†æå¤±åœ¨ 15 è¼ªå·¦å³å°±è¶¨æ–¼ç©©å®šï¼Œèªªæ˜Žæ¨¡åž‹ç„¡æ³•å¾ˆå¥½åœ°æŽ¨å»£åˆ°é©—è­‰æ•¸æ“šã€‚

- **ç¸½çµåˆ†æž**  
  ç¸½çš„ä¾†èªªï¼Œé€™å€‹æ¨¡åž‹ç¢ºå¯¦æ•æ‰åˆ°äº†è¨“ç·´è³‡æ–™ä¸­çš„ä¸€äº›ç›¸é—œæ¨¡å¼ï¼Œä½†åœ¨æ³›åŒ–æ€§èƒ½æ–¹é¢é‚„éœ€è¦é€²ä¸€æ­¥å„ªåŒ–ã€‚

## å¯ä»¥å˜—è©¦ä»¥ä¸‹å¹¾ç¨®æ–¹æ³•

1. **èª¿æ•´æ¨¡åž‹æž¶æ§‹**  
   æ¸›å°‘æ¨¡åž‹è¤‡é›œåº¦ä¾†é˜²æ­¢éŽåº¦æ“¬åˆã€‚

2. **æ­£è¦åŒ–æŠ€è¡“**  
   å˜—è©¦ä¸åŒçš„æ­£è¦åŒ–æŠ€è¡“ï¼Œå¦‚ Dropout æˆ– L1/L2 æ­£å‰‡åŒ–ã€‚

3. **å¢žåŠ è³‡æ–™å¤šæ¨£æ€§**  
   å¢žåŠ è¨“ç·´è³‡æ–™çš„æ•¸é‡å’Œå¤šæ¨£æ€§ï¼Œä»¥æé«˜æ³›åŒ–èƒ½åŠ›ã€‚

4. **è¶…åƒæ•¸èª¿æ•´**  
   èª¿æ•´å­¸ç¿’çŽ‡ã€æ‰¹é‡å¤§å°ç­‰è¶…åƒæ•¸é€²è¡Œå„ªåŒ–ã€‚

## ç¸½çµ

- é—œéµåœ¨æ–¼ **æ¨¡åž‹è¤‡é›œåº¦** èˆ‡ **æ³›åŒ–æ€§èƒ½** ä¹‹é–“æ‰¾åˆ°å¹³è¡¡ã€‚
- é€šéŽä¸€äº›ç´°ç·»çš„èª¿æ•´ï¼Œç›¸ä¿¡æ¨¡åž‹çš„é©—è­‰é›†æº–ç¢ºçŽ‡å’Œæå¤±éƒ½èƒ½å¾—åˆ°é€²ä¸€æ­¥æå‡ã€‚
ï¼Œç›¸ä¿¡æ¨¡åž‹çš„é©—è­‰é›†æº–ç¢ºçŽ‡å’Œæå¤±éƒ½èƒ½å¾—åˆ°é€²ä¸€æ­¥æå‡ã€‚
```æº–ç¢ºçŽ‡å’Œæå¤±éƒ½èƒ½å¾—åˆ°é€²ä¸€æ­¥çš„æå‡ã€‚å’Œæå¤±éƒ½èƒ½å¾—åˆ°é€²ä¸€æ­¥çš„æå‡ã€‚
# Exercise 6, 12 (ç¥žç¶“ç¶²è·¯èˆ‡ KNN æ¨¡åž‹çµæžœåˆ†æž)

## 1. æ¨¡åž‹æ€§èƒ½æ¯”è¼ƒ

### 1.1 æ¨¡åž‹æº–ç¢ºæ€§å°æ¯”

| æ¨¡åž‹ | æº–ç¢ºçŽ‡ | ä¸»è¦ç‰¹é»ž |
|------|--------|----------|
| KNN  | 45.82%       | åŸºæ–¼è·é›¢çš„åˆ†é¡žå™¨ |
| ç¥žç¶“ç¶²è·¯ | 57%    | åŸºæ–¼ LLM åµŒå…¥çš„å¤šå±¤æ„ŸçŸ¥æ©Ÿ |

å¾žæ•¸æ“šå¯ä»¥çœ‹å‡ºï¼Œç¥žç¶“ç¶²è·¯æ¨¡åž‹æ¯” KNN æ¨¡åž‹çš„æº–ç¢ºçŽ‡é«˜ç´„ 12.47 å€‹ç™¾åˆ†é»žï¼Œé¡¯ç¤ºä½¿ç”¨ LLM åµŒå…¥å’Œç¥žç¶“ç¶²è·¯åœ¨æƒ…ç·’åˆ†é¡žä»»å‹™ä¸Šå…·æœ‰æ˜Žé¡¯å„ªå‹¢ã€‚

### 1.2 æ€§èƒ½è©•ä¼°æŒ‡æ¨™

- **æº–ç¢ºçŽ‡**ï¼šæ¨¡åž‹æ­£ç¢ºåˆ†é¡žçš„æ¨£æœ¬æ¯”ä¾‹
- **F1 åˆ†æ•¸**ï¼šç²¾ç¢ºçŽ‡å’Œå¬å›žçŽ‡çš„èª¿å’Œå¹³å‡å€¼
- **æ··æ·†çŸ©é™£**ï¼šé¡¯ç¤ºæ¯å€‹é¡žåˆ¥çš„åˆ†é¡žçµæžœ

## 2. è¨“ç·´éŽç¨‹åˆ†æžï¼ˆExercise 6ï¼‰

### 2.1 æº–ç¢ºæ€§æ›²ç·š

- è¨“ç·´æº–ç¢ºæ€§æŒçºŒä¸Šå‡
- é©—è­‰æº–ç¢ºæ€§åŒæ­¥æå‡
- èªªæ˜Žæ¨¡åž‹æœ‰æ•ˆå­¸ç¿’æ•¸æ“šç‰¹å¾µ

### 2.2 æå¤±æ›²ç·š

- è¨“ç·´å’Œé©—è­‰æå¤±ç©©æ­¥ä¸‹é™
- æ¨¡åž‹é æ¸¬èª¤å·®é€æ­¥æ¸›å°
- å­¸ç¿’éŽç¨‹ç›¸å°ç©©å®š

## 3. ä½¿ç”¨ LLM åµŒå…¥çš„å„ªå‹¢

### 3.1 èªžç¾©è¡¨ç¤º

- å‚³çµ±æ–¹æ³•ï¼š
 - åƒ…æ•ç²è¡¨é¢æ–‡æœ¬ç‰¹å¾µ
 - å¿½ç•¥ä¸Šä¸‹æ–‡å’Œæ·±å±¤èªžç¾©

- LLM åµŒå…¥ï¼š
 - æä¾›é«˜ç¶­ã€èªžç¾©è±å¯Œçš„å‘é‡è¡¨ç¤º
 - æ•æ‰è¤‡é›œçš„èªžè¨€èªžç¾©å’Œä¸Šä¸‹æ–‡é—œä¿‚

### 3.2 ç‰¹å¾µæ“·å–èƒ½åŠ›

1. **æ·±å±¤èªžç¾©ç†è§£**
  - è­˜åˆ¥æ–‡æœ¬ä¸­çš„æ½›åœ¨æƒ…æ„Ÿå¾®å¦™ä¹‹è™•
  - è¶…è¶Šå‚³çµ±ç‰¹å¾µæå–æ–¹æ³•çš„è¡¨å¾µèƒ½åŠ›

2. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**
  - è€ƒæ…®æ•´é«”èªžå¢ƒï¼Œè€Œéžå­¤ç«‹è©žèªž
  - æ›´å¥½åœ°ç†è§£æƒ…æ„Ÿçš„è¤‡é›œæ€§

### 3.3 æ¨¡åž‹å­¸ç¿’å„ªå‹¢

- ç¥žç¶“ç¶²è·¯å¯ä»¥å­¸ç¿’ LLM åµŒå…¥ä¸­çš„éžç·šæ€§ç‰¹å¾µ
- å¤šå±¤ç¶²è·¯çµæ§‹æå–æ›´æŠ½è±¡çš„ç‰¹å¾µè¡¨ç¤º
- ç›¸æ¯” KNN çš„ç·šæ€§æ±ºç­–é‚Šç•Œï¼Œå…·æœ‰æ›´å¼·çš„åˆ†é¡žèƒ½åŠ›

## 4. ä½¿ç”¨ LLM åµŒå…¥çš„é—œéµå„ªé»ž

- æ›´é«˜çš„èªžç¾©ç†è§£èƒ½åŠ›
- è±å¯Œçš„ç‰¹å¾µè¡¨ç¤º
- è¤‡é›œçš„éžç·šæ€§æ±ºç­–é‚Šç•Œ
- ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æƒ…æ„Ÿåˆ†é¡ž

## 5. æ½›åœ¨æ”¹é€²æ–¹å‘

- èª¿æ•´ç¶²è·¯æž¶æ§‹
- å˜—è©¦ä¸åŒçš„ LLM åµŒå…¥æ¨¡åž‹
- å„ªåŒ–è¶…åƒæ•¸
- å¢žåŠ è¨“ç·´æ•¸æ“šå¤šæ¨£æ€§

## 6. çµè«–

**LLM åµŒå…¥é¡¯è‘—æå‡äº†æƒ…æ„Ÿåˆ†é¡žæ¨¡åž‹çš„æ€§èƒ½ï¼Œé€šéŽæ•æ‰æ·±å±¤èªžç¾©å’Œè¤‡é›œä¸Šä¸‹æ–‡ï¼Œå±•ç¾å‡ºæ¯”å‚³çµ±æ–¹æ³•æ›´å„ªè¶Šçš„åˆ†é¡žèƒ½åŠ›ã€‚**
## 10.5 Few-Shot Prompting Classification:
Few-shot prompting classification for LLMs involves giving the model a few labeled examples (typically 5 or fewer) within a prompt to guide it in performing a classification task. Instead of needing extensive training, the LLM uses these examples to understand the task and classify new inputs. This approach is significant in current research because it allows LLMs to perform well on tasks with minimal labeled data, reducing the need for large training datasets and making it highly flexible for various NLP tasks, including those in low-resource languages or niche domains.

In this lab exercise, we will explore zero-shot, 1-shot, and 5-shot prompting for classification using an LLM:

Zero-shot means the model performs classification without seeing any examples beforehand.
1-shot provides the model with just one labeled example per class to guide its classification.
5-shot gives the model five labeled examples per class to improve its understanding of the task.
Since processing large datasets can be computationally demanding, we will only sample 20 test texts per emotion for the classification task, allowing us to test the model's performance more efficiently without using the entire test set.

Process order: Explanation Prompt -> Examples + labels (if it is not zero-shot) -> Text to classify

Recommendation for the explanation prompt: Explain to the model that it is a classification model of certain labels, and to only output the label word, and no other explanation. In this case if the model does not follow the instructions we are retrying the same text until it outputs one of the accepted labels for it.
import ollama
import random
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix

# Define the emotion labels
emotions = ['anger', 'fear', 'joy', 'sadness']
# Define the model to use for few-shot prompting
model_ollama = "llama3.2"

# Function to sample examples per emotion category
def sample_few_shots(df, emotions, num_samples=5):
    few_shot_examples = {}
    for emotion in emotions:
        few_shot_examples[emotion] = df[df['emotion'] == emotion].sample(n=num_samples, random_state=42)
    return few_shot_examples

# Function to build the prompt based on the number of examples (few-shot, 1-shot, zero-shot)
def build_prompt(examples, emotions, num_shots=5):
    classification_instructions = """
You are an emotion classification model. You will be given a text from social media and your task is to classify the text into one of the following emotion categories: anger, fear, joy, or sadness.
You must only output one of these four labels. Do not provide any additional information or explanation. Just output the emotion label as one word.
    """

    prompt = classification_instructions + "\n\n"

    if num_shots > 0:
        prompt += f"Examples: \n"
        for emotion in emotions:
            for _, row in examples[emotion].iterrows():
                prompt += f"Text: {row['text']}\nEmotion: {emotion}\n\n" #Show the examples in the same format it will be shown for the classification text
                if num_shots == 1:  # If 1-shot, break after the first example for each emotion
                    break
    return prompt

# Function to classify using the LLM with retry for incorrect responses
def classify_with_llm(test_text, prompt_base):
    valid_emotions = ['anger', 'fear', 'joy', 'sadness']
    response = None
    while not response or response not in valid_emotions:
        full_prompt = f"{prompt_base}\nClassification:\nText: {test_text}\nEmotion: " #The classification text will leave the emotion label to be filled in by the LLM
        result = ollama.chat(model=model_ollama, messages=[
            {'role': 'user', 'content': full_prompt}
        ])
        response = result['message']['content'].strip().lower()  # Clean and standardize the response
        if response not in valid_emotions:  # Retry if not a valid response
            print(f"Invalid response: {response}. Asking for reclassification.")
    return response

# Main function to run the experiment with the option for zero-shot, 1-shot, or 5-shot prompting
def run_experiment(df_train, df_test, test_samples=5, num_shots=5):
    # Sample examples for few-shot prompting based on num_shots
    if num_shots > 0:
        few_shot_examples = sample_few_shots(df_train, emotions, num_samples=num_shots)
        prompt_base = build_prompt(few_shot_examples, emotions, num_shots=num_shots)
    else:
        prompt_base = build_prompt(None, emotions, num_shots=0)  # Zero-shot has no examples

    # Prepare to classify the test set
    predictions = []
    true_labels = []
    print(prompt_base)
    # Sample 20 examples per emotion for the test set to classify
    test_samples = sample_few_shots(df_test, emotions, num_samples=test_samples)

    # Classify 20 test examples (5 from each category) and save predictions
    for emotion in emotions:
        for _, test_row in test_samples[emotion].iterrows():
            test_text = test_row['text']
            predicted_emotion = classify_with_llm(test_text, prompt_base)
            predictions.append(predicted_emotion)
            true_labels.append(emotion)

    # Calculate accuracy
    accuracy = accuracy_score(true_labels, predictions)
    print(f"Accuracy: {accuracy * 100:.2f}%")

    # Classification report
    print(classification_report(y_true=true_labels, y_pred=predictions))

    # Plot confusion matrix
    cm = confusion_matrix(y_true=true_labels, y_pred=predictions)
    my_tags = ['anger', 'fear', 'joy', 'sadness']
    plot_confusion_matrix(cm, classes=my_tags, title=f'Confusion matrix for classification with \n{num_shots}-shot prompting')
# Example of running the experiment with zero-shot prompting
run_experiment(df_train, df_test, test_samples=20, num_shots=0)
# Example of running the experiment with 1-shot prompting
run_experiment(df_train, df_test, test_samples=20, num_shots=1)
# Example of running the experiment with 5-shot prompting
run_experiment(df_train, df_test, test_samples=20, num_shots=5)
## ** >>> Exercise 13 (Take home): **
Compare and discuss the results of the zero-shot, 1-shot and 5-shot classification.
# Answer here
# ä¸€ã€é›¶æ¨£æœ¬ï¼ˆZero-shotï¼‰åˆ†é¡žçµæžœåˆ†æž
## 1. å„ªé»ž
- ä¸éœ€æä¾›ä»»ä½•ç¤ºä¾‹ï¼Œå®Œå…¨ä¾è³´æ¨¡åž‹çš„èªžè¨€ç†è§£èƒ½åŠ›ã€‚
- é©åˆæ–¼æœªçŸ¥é ˜åŸŸæˆ–é›£ä»¥æº–å‚™æ¨£æœ¬çš„å ´æ™¯ã€‚

## 2. ç¼ºé»ž
- çµæžœé€šå¸¸ä¸å¤ ç²¾ç¢ºï¼Œå°¤å…¶åœ¨èªžæ„æ¨¡ç³Šæˆ–ä»»å‹™è¤‡é›œçš„æƒ…æ³ä¸‹ã€‚
- å®¹æ˜“å—åˆ°æ¨¡åž‹é è¨“ç·´è³‡æ–™çš„é™åˆ¶ï¼Œå°Žè‡´åå·®ã€‚

## 3. æ‡‰ç”¨å ´æ™¯
- åˆæ­¥æŽ¢ç´¢æ–°å•é¡Œï¼Œç„¡éœ€æ¨™è¨»è³‡æ–™çš„å¿«é€Ÿæ¸¬è©¦ã€‚

---

# äºŒã€ä¸€æ¨£æœ¬ï¼ˆ1-shotï¼‰åˆ†é¡žçµæžœåˆ†æž
## 1. å„ªé»ž
- æä¾›ä¸€å€‹ç¯„ä¾‹å¯é¡¯è‘—æé«˜æ¨¡åž‹å°ä»»å‹™çš„ç†è§£èƒ½åŠ›ã€‚
- é©åˆä½Žæ•¸æ“šå ´æ™¯ï¼Œä¿ç•™ä¸€å®šçš„éˆæ´»æ€§ã€‚

## 2. ç¼ºé»ž
- å–®ä¸€ç¯„ä¾‹å¯èƒ½ç„¡æ³•æ¶µè“‹å¤šæ¨£åŒ–çš„æƒ…å¢ƒï¼Œå°Žè‡´è¡¨ç¾ä¸ç©©å®šã€‚
- ç•¶ç¯„ä¾‹é¸æ“‡ä¸ä½³æ™‚ï¼Œæ•ˆæžœå¯èƒ½æŽ¥è¿‘é›¶æ¨£æœ¬ã€‚

## 3. æ‡‰ç”¨å ´æ™¯
- åœ¨ç„¡æ³•ç²å¾—å¤§é‡æ•¸æ“šæ™‚ï¼Œå¿«é€Ÿæ¸¬è©¦æ¨¡åž‹æ€§èƒ½ã€‚

---

# ä¸‰ã€äº”æ¨£æœ¬ï¼ˆ5-shotï¼‰åˆ†é¡žçµæžœåˆ†æž
## 1. å„ªé»ž
- æä¾›æ›´å¤šç¤ºä¾‹ï¼Œæ¨¡åž‹èƒ½æ›´æº–ç¢ºåœ°æ•æ‰ä»»å‹™ç‰¹å¾µã€‚
- æ¸›å°‘äº†ç¯„ä¾‹é¸æ“‡çš„åå·®å½±éŸ¿ï¼Œçµæžœæ›´ç©©å®šã€‚

## 2. ç¼ºé»ž
- å¢žåŠ çš„æ¨£æœ¬æ•¸é‡å¸¶ä¾†æ›´å¤šæç¤ºæˆæœ¬ï¼Œå°è¨ˆç®—è³‡æºéœ€æ±‚æ›´é«˜ã€‚
- åœ¨ç¤ºä¾‹ä¸å…·ä»£è¡¨æ€§çš„æƒ…æ³ä¸‹ï¼Œæ€§èƒ½æå‡æœ‰é™ã€‚

## 3. æ‡‰ç”¨å ´æ™¯
- æœ‰è¶³å¤ çš„æç¤ºç©ºé–“åŠè³‡æºæ™‚ï¼Œç”¨æ–¼ç´°åŒ–æ¨¡åž‹è¡¨ç¾ã€‚

---

# å››ã€æ•´é«”æ¯”è¼ƒèˆ‡å»ºè­°
## 1. æ€§èƒ½æ¯”è¼ƒ
- äº”æ¨£æœ¬åˆ†é¡žé€šå¸¸å„ªæ–¼ä¸€æ¨£æœ¬å’Œé›¶æ¨£æœ¬ï¼Œä½†å¢žé•·å¹…åº¦ä¾ä»»å‹™è¤‡é›œæ€§å’Œæ¨¡åž‹èƒ½åŠ›è€Œç•°ã€‚

## 2. è³‡æºæˆæœ¬
- é›¶æ¨£æœ¬æ•ˆçŽ‡æœ€é«˜ï¼Œä½†çµæžœä¸ç©©å®šï¼›äº”æ¨£æœ¬ç©©å®šä½†æç¤ºæˆæœ¬è¼ƒé«˜ã€‚

## 3. å»ºè­°é¸æ“‡
- æ ¹æ“šå ´æ™¯éœ€æ±‚ï¼Œéˆæ´»é¸ç”¨ã€‚å°æ–¼é«˜ç²¾åº¦è¦æ±‚çš„ä»»å‹™ï¼Œå»ºè­°è‡³å°‘æä¾›1-5å€‹æ¨£æœ¬ä»¥æå‡æ¨¡åž‹æ€§èƒ½ã€‚


---
# ä»¥ä¸Šæ˜¯ç¬¬ 1 éƒ¨åˆ†

